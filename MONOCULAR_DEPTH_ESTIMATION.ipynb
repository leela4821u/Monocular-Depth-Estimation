{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leela4821u/Monocular-Depth-Estimation/blob/main/MONOCULAR_DEPTH_ESTIMATION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BP5ctMnmENXm"
      },
      "source": [
        "# Monocular depth estimation\n",
        "\n",
        "**Author:** [Irlanki Leela Sai Abhiram](lirlanki.com)<br>"
        "**Contributor:** [Sandip Paul](sandippaul@live.com)<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "397Yw0NrENX4"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "_Depth estimation_ estimates depth scene geometry from 2D images.\n",
        "In _monocular depth estimation_ the depth is predicted from a single RGB image.Here the research is on This example will show an approach to build a depth estimation model with a convnet\n",
        "and simple loss functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g9Sq0lAENX8"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7m3PoFOAekp_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pc2DpZgN2I4I"
      },
      "outputs": [],
      "source": [
        "  %%capture\n",
        "!pip install tensorflow_addons\n",
        "!pip install pyyaml h5py  # Required to save models in HDF5 format\n",
        "!nvidia-smi\n",
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()\n",
        "#!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1fdFu5NGXe4rTLYKD5wOqk9dl-eJOefXo' -O nyu_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fquynj-iENX_"
      },
      "outputs": [],
      "source": [
        "import os, sys, time, pathlib, os.path, argparse, glob\n",
        "from zipfile import ZipFile\n",
        "from io import BytesIO\n",
        "from datetime import datetime\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.utils import normalize\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import Input, concatenate, ReLU, LeakyReLU, BatchNormalization, Conv2D, SeparableConv2D, MaxPooling2D, Dropout, UpSampling2D, Conv2DTranspose, add, multiply, Activation, Lambda\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.losses import Loss\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow_addons as tfa\n",
        "#import tensorflow.keras as keras\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import numpy\n",
        "import pywt\n",
        "import csv\n",
        "import PIL\n",
        "import imutils\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageEnhance, ImageOps\n",
        "from skimage.transform import resize\n",
        "\n",
        "import random\n",
        "tf.random.set_seed(123)\n",
        "\n",
        "import keras.backend as K\n",
        "# from keras.layers import Layer\n",
        "import keras\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.layers import Input, Conv2D, SeparableConv2D, ZeroPadding2D, UpSampling2D, Dense, concatenate, Conv2DTranspose\n",
        "from keras.layers import BatchNormalization, Dropout, Flatten, Lambda, Input, Conv2D, Add\n",
        "#from keras.layers.pooling import MaxPooling2D, GlobalAveragePooling2D, MaxPooling2D\n",
        "#from keras.layers.core import Dense, Dropout, Activation\n",
        "#from keras.layers.advanced_activations import ELU, LeakyReLU\n",
        "#from keras.optimizers import Adam, RMSprop, SGD\n",
        "from keras.regularizers import l2\n",
        "#from keras.layers.noise import GaussianDropout\n",
        "keras.saving.get_custom_objects().clear()\n",
        "print(tf.version.VERSION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI2jdzc1eKxB"
      },
      "source": [
        "## Preparing hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLY9Psodbt-i"
      },
      "outputs": [],
      "source": [
        "RGB_H = 480\n",
        "RGB_W = 640\n",
        "RGB_C = 3\n",
        "Dep_H = 480\n",
        "Dep_W = 640\n",
        "Dep_C = 1\n",
        "\n",
        "EPOCHS =10\n",
        "BATCH_SIZE = 4\n",
        "# HEIGHT = 256\n",
        "# WIDTH = 256\n",
        "# LR = 0.001 #.0002\n",
        "dropout_rate = 0.5\n",
        "HEIGHT = 240\n",
        "WIDTH = 320\n",
        "INIT_LR = 0.0001\n",
        "TRAIN_PATH = '/content/data/nyu2_train.csv'\n",
        "TEST_PATH = '/content/data/nyu2_test.csv'\n",
        "checkpoint_path = \"/content/drive/MyDrive/NDWT/cp.ckpt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hlGOwVyENYG"
      },
      "source": [
        "## Downloading the dataset\n",
        "\n",
        "Training and evaluation datasets are **DIODE: A Dense Indoor and Outdoor Depth Dataset** and **[NYU-v2](https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html)**. Outdoor dataset **[KITTI](http://www.cvlibs.net/datasets/kitti/)** will be tried in course of time.  We use the validation set from DIODE for generating training and evaluation subsets\n",
        "for our model. The reason we use the validation set rather than the training set of the original dataset is because\n",
        "the training set consists of 81GB of data, which is challenging to download compared\n",
        "to the validation set which is only 2.6GB. This gives a fast check for the models. For NYU we divide training, validation amd evaluation dataset as given by authors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6E0p5rdENYO"
      },
      "source": [
        "##  Preparing the DIODE dataset\n",
        "\n",
        "We only use the indoor images to train our depth estimation model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4xjO8PJENYJ"
      },
      "outputs": [],
      "source": [
        "annotation_folder = \"/content/drive/MyDrive/colab/\"\n",
        "if not os.path.exists(os.path.abspath(\".\") + annotation_folder):\n",
        "    annotation_zip = tf.keras.utils.get_file(\n",
        "        \"val.tar.gz\",\n",
        "        cache_subdir=os.path.abspath(\".\"),\n",
        "        origin=\"http://diode-dataset.s3.amazonaws.com/val.tar.gz\",\n",
        "        extract=True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_5MJiPxENYS"
      },
      "outputs": [],
      "source": [
        "path = \"val/indoors\"\n",
        "\n",
        "filelist = []\n",
        "\n",
        "for root, dirs, files in os.walk(path):\n",
        "    for file in files:\n",
        "        filelist.append(os.path.join(root, file))\n",
        "\n",
        "filelist.sort()\n",
        "data = {\n",
        "    \"image\": [x for x in filelist if x.endswith(\".png\")],\n",
        "    \"depth\": [x for x in filelist if x.endswith(\"_depth.npy\")],\n",
        "    #\"mask\": [x for x in filelist if x.endswith(\"_depth_mask.npy\")],\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df = df.sample(frac=1, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYkSYlY2ENYg"
      },
      "source": [
        "## Building Diode data pipeline\n",
        "\n",
        "1. The pipeline takes a dataframe containing the path for the RGB images,\n",
        "as well as the depth and depth mask files.\n",
        "2. It reads and resize the RGB images (1024,768) -->(320,240).\n",
        "3. It reads the depth and depth mask files, process them to generate the depth map image and\n",
        "resize it.\n",
        "4. It returns the RGB images and the depth map images for a batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5M35m565ENYi"
      },
      "outputs": [],
      "source": [
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, data, batch_size=6, dim=(768, 1024), n_channels=3, shuffle=True): #768, 1024\n",
        "        \"\"\"\n",
        "        Initialization\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.indices = self.data.index.tolist()\n",
        "        self.dim = dim\n",
        "        self.n_channels = n_channels\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.min_depth = 0.1\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.data) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # if (index + 1) * self.batch_size > len(self.indices):\n",
        "        #     self.batch_size = len(self.indices) - index * self.batch_size\n",
        "        # # Generate one batch of data\n",
        "        # # Generate indices of the batch\n",
        "        # index = self.indices[index * self.batch_size : (index + 1) * self.batch_size]\n",
        "        # # Find list of IDs\n",
        "        # batch = [self.indices[k] for k in index]\n",
        "        # x, y = self.data_generation(batch)\n",
        "\n",
        "        # return x, y\n",
        "        start_idx = index * self.batch_size\n",
        "        end_idx = min((index + 1) * self.batch_size, len(self.indices))\n",
        "        batch_indices = self.indices[start_idx:end_idx]\n",
        "\n",
        "        x, y = self.data_generation(batch_indices)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "\n",
        "        \"\"\"\n",
        "        Updates indexes after each epoch\n",
        "        \"\"\"\n",
        "        self.index = np.arange(len(self.indices))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.index)\n",
        "\n",
        "    def load(self, image_path, depth_map):  #, mask\n",
        "        \"\"\"Load input and target image.\"\"\"\n",
        "\n",
        "        image_ = cv2.imread(image_path)\n",
        "        image_ = cv2.cvtColor(image_, cv2.COLOR_BGR2RGB)\n",
        "        image_ = cv2.resize(image_, (320,240))\n",
        "        image_ = tf.image.convert_image_dtype(image_, tf.float32)\n",
        "\n",
        "        depth_map = np.load(depth_map).squeeze()\n",
        "\n",
        "        #mask = np.load(mask)\n",
        "        #mask = mask > 0\n",
        "\n",
        "        max_depth = min(300, np.percentile(depth_map, 99))\n",
        "        depth_map = np.clip(depth_map, self.min_depth, max_depth)\n",
        "        depth_map = np.log(depth_map) #, where=mask)\n",
        "\n",
        "        #depth_map = np.ma.masked_where(~mask, depth_map)\n",
        "\n",
        "        depth_map = np.clip(depth_map, 0.1, np.log(max_depth))\n",
        "        depth_map = cv2.resize(depth_map, (320,240))\n",
        "        depth_map = np.expand_dims(depth_map, axis=2)\n",
        "        depth_map = tf.image.convert_image_dtype(depth_map, tf.float32)\n",
        "\n",
        "        return image_, depth_map\n",
        "\n",
        "    def data_generation(self, batch):\n",
        "\n",
        "        x = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
        "        y = np.empty((self.batch_size, *self.dim, 1))\n",
        "\n",
        "        for i, batch_id in enumerate(batch):\n",
        "            x[i,], y[i,] = self.load(\n",
        "                self.data[\"image\"][batch_id],\n",
        "                self.data[\"depth\"][batch_id],\n",
        "                #self.data[\"mask\"][batch_id],\n",
        "            )\n",
        "\n",
        "        return x, y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_alyQ5YdENYp"
      },
      "source": [
        "## Visualizing samples (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uj1J_kMwENYr"
      },
      "outputs": [],
      "source": [
        "\n",
        "def visualize_depth_map(samples, test=False, model=None):\n",
        "    input, target = samples\n",
        "    cmap = plt.cm.jet\n",
        "    cmap.set_bad(color=\"black\")\n",
        "    Sa=10\n",
        "    if test:\n",
        "        print(\"PRinting model \")\n",
        "        pred = model.predict(input)\n",
        "        print(\"PRinting model \")\n",
        "        fig, ax = plt.subplots(Sa, 3, figsize=(50, 50))\n",
        "        for i in range(Sa):\n",
        "            ax[i, 0].imshow((input[i].squeeze()))\n",
        "            ax[i, 1].imshow((target[i].squeeze()), cmap=cmap)\n",
        "            ax[i, 2].imshow((pred[i].squeeze()), cmap=cmap)\n",
        "\n",
        "    else:\n",
        "        print(\"PRinting just model \")\n",
        "        fig, ax = plt.subplots(Sa, 2, figsize=(50, 50))\n",
        "        for i in range(Sa):\n",
        "            ax[i, 0].imshow((input[i].squeeze()))\n",
        "            ax[i, 1].imshow((target[i].squeeze()), cmap=cmap)\n",
        "\n",
        "\n",
        "# visualize_samples = next(\n",
        "#     iter(DataGenerator(data=df, batch_size=15, dim=(HEIGHT, WIDTH)))\n",
        "# )\n",
        "# visualize_depth_map(visualize_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "BD8H8Sn0APyq"
      },
      "outputs": [],
      "source": [
        "test_loader = next(\n",
        "    iter(\n",
        "        DataGenerator(\n",
        "            data=df[265:].reset_index(drop=\"true\"), batch_size=5, dim=(HEIGHT, WIDTH)\n",
        "        )\n",
        "    )\n",
        ")\n",
        "visualize_depth_map(test_loader, test=True, model=model)\n",
        "\n",
        "# test_loader = next(\n",
        "#     iter(\n",
        "#         DataGenerator(\n",
        "#             data=df[300:].reset_index(drop=\"true\"), batch_size=6, dim=(HEIGHT, WIDTH)\n",
        "#         )\n",
        "#     )\n",
        "# )\n",
        "# visualize_depth_map(test_loader, test=True, model=model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## evaluation for diode dataset"
      ],
      "metadata": {
        "id": "07C9rmLKX3-i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2kH78BVPvBv"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.load_model(\n",
        "    '/content/drive/MyDrive/colab/saved_models/keras/prunedNestNet8.keras',\n",
        "    safe_mode=False  # Allow Lambda layer loading\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model('/content/drive/MyDrive/colab/saved_models/filtDWTunet10.keras')"
      ],
      "metadata": {
        "id": "u3nXiuKol_2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDKfJhqdPSpq",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "def calculate_metrics(true_depth, predicted_depth):\n",
        "    \"\"\"Calculate evaluation metrics for depth estimation.\"\"\"\n",
        "    true_depth_flat = true_depth.flatten()\n",
        "    predicted_depth_flat = predicted_depth.flatten()\n",
        "\n",
        "    # Mean Absolute Error\n",
        "    mae = mean_absolute_error(true_depth_flat, predicted_depth_flat)\n",
        "\n",
        "    # Root Mean Squared Error\n",
        "    rmse = np.sqrt(mean_squared_error(true_depth_flat, predicted_depth_flat))\n",
        "\n",
        "    # Threshold metrics\n",
        "    threshold_1 = np.mean((np.maximum(true_depth_flat / predicted_depth_flat, predicted_depth_flat / true_depth_flat)) < 1.25)\n",
        "    threshold_2 = np.mean((np.maximum(true_depth_flat / predicted_depth_flat, predicted_depth_flat / true_depth_flat)) < 1.25**2)\n",
        "    threshold_3 = np.mean((np.maximum(true_depth_flat / predicted_depth_flat, predicted_depth_flat / true_depth_flat)) < 1.25**3)\n",
        "\n",
        "    return mae, rmse, threshold_1, threshold_2, threshold_3\n",
        "\n",
        "def evaluate_model(model, data, batch_size):\n",
        "    \"\"\"Evaluate a model on a dataset and return the metrics.\"\"\"\n",
        "    num_samples = len(data)\n",
        "    num_batches = int(np.ceil(num_samples / batch_size))\n",
        "\n",
        "    mae_list, rmse_list, thr1_list, thr2_list, thr3_list = [], [], [], [], []\n",
        "    total_images = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        batch_data = data[i * batch_size:(i + 1) * batch_size]\n",
        "        data_loader = next(iter(DataGenerator(data=batch_data, batch_size=batch_size, dim=(HEIGHT, WIDTH))))\n",
        "        input, target = data_loader\n",
        "\n",
        "        pred = model.predict(input)\n",
        "        batch_size_actual = len(input)\n",
        "        total_images += batch_size_actual\n",
        "\n",
        "        for j in range(len(input)):\n",
        "            mae, rmse, thr1, thr2, thr3 = calculate_metrics(target[j].squeeze(), pred[j].squeeze())\n",
        "            mae_list.append(mae)\n",
        "            rmse_list.append(rmse)\n",
        "            thr1_list.append(thr1)\n",
        "            thr2_list.append(thr2)\n",
        "            thr3_list.append(thr3)\n",
        "    end_time = time.time()\n",
        "    evaluation_time = end_time - start_time\n",
        "\n",
        "    mean_mae = np.mean(mae_list)\n",
        "    mean_rmse = np.mean(rmse_list)\n",
        "    mean_thr1 = np.mean(thr1_list)\n",
        "    mean_thr2 = np.mean(thr2_list)\n",
        "    mean_thr3 = np.mean(thr3_list)\n",
        "\n",
        "    return mean_mae, mean_rmse, mean_thr1, mean_thr2, mean_thr3, total_images, evaluation_time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 12\n",
        "data = df[100:300].reset_index(drop=True)\n",
        "\n",
        "model_name = \"filtDWTunet10\"\n",
        "mae, rmse, thr1, thr2, thr3, total_images, evaluation_time = evaluate_model(model, data, batch_size)\n",
        "\n",
        "# Print results directly\n",
        "print(f\"Results for {model_name}:\")\n",
        "print(f\"MAE: {mae:.4f}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "print(f\"Threshold < 1.25: {thr1:.4f}\")\n",
        "print(f\"Threshold < 1.25²: {thr2:.4f}\")\n",
        "print(f\"Threshold < 1.25³: {thr3:.4f}\")\n",
        "print(f\"Total images evaluated: {total_images}\")\n",
        "print(f\"Evaluation time: {evaluation_time:.2f} seconds\")\n",
        "print()\n"
      ],
      "metadata": {
        "id": "7hoFd-QnYMTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mizuXicvGsOK"
      },
      "outputs": [],
      "source": [
        "print(new_model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnXeQzbdENYu"
      },
      "source": [
        "## 3D point cloud visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7XufTIMXENYv"
      },
      "outputs": [],
      "source": [
        "depth_vis = np.flipud(visualize_samples[1][1].squeeze())  # target\n",
        "img_vis = np.flipud(visualize_samples[0][1].squeeze())  # input\n",
        "\n",
        "fig = plt.figure(figsize=(15, 10))\n",
        "ax = plt.axes(projection=\"3d\")\n",
        "\n",
        "STEP = 3\n",
        "for x in range(0, img_vis.shape[0], STEP):\n",
        "    for y in range(0, img_vis.shape[1], STEP):\n",
        "        ax.scatter(\n",
        "            [depth_vis[x, y]] * 3,\n",
        "            [y] * 3,\n",
        "            [x] * 3,\n",
        "            c=tuple(img_vis[x, y, :3] / 255),\n",
        "            s=3,\n",
        "        )\n",
        "    ax.view_init(45, 135)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbyoA5vfqcUj"
      },
      "outputs": [],
      "source": [
        "training_generator = DataGenerator(\n",
        "    data=df[:260].reset_index(drop=\"true\"), batch_size=BATCH_SIZE, dim=(HEIGHT, WIDTH)\n",
        ")\n",
        "validation_generator = DataGenerator(\n",
        "    data=df[260:].reset_index(drop=\"true\"), batch_size=BATCH_SIZE, dim=(HEIGHT, WIDTH)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I0ubMh0rtD5"
      },
      "source": [
        "#Load NYU Data and build data pipeline (either DIODE or this)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDoKPnAc2rrl"
      },
      "outputs": [],
      "source": [
        "!git clone https://gitlab.com/siddinc/new_depth.git ./data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2BHhG252x8H"
      },
      "outputs": [],
      "source": [
        "# loading the dataset\n",
        "\n",
        "def read_csv(csv_file_path):\n",
        "  with open(csv_file_path, 'r') as f:\n",
        "    csv_reader = csv.reader(f, delimiter=',')\n",
        "    return [('./' + row[0], './' + row[1]) for row in csv_reader if len(row) > 0]\n",
        "\n",
        "def train_val_split(train_paths, val_size):\n",
        "  random.shuffle(train_paths)\n",
        "  len_train_paths = len(train_paths)\n",
        "  i = int(len_train_paths*(1.0 - val_size))\n",
        "  train = train_paths[0:i]\n",
        "  val = train_paths[i:len(train_paths)]\n",
        "  return train, val\n",
        "\n",
        "def load_train_paths(train_path):\n",
        "  train_paths = read_csv(train_path)\n",
        "  labels = {img_path: dm_path for img_path, dm_path in train_paths}\n",
        "  x_paths = [img_path for img_path, dm in train_paths]\n",
        "  x_train_paths, x_val_paths = train_val_split(x_paths, 0.3)\n",
        "\n",
        "  partition = {\n",
        "    'train': x_train_paths,\n",
        "    'validation': x_val_paths\n",
        "  }\n",
        "  return partition, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJwN17y92zc1"
      },
      "outputs": [],
      "source": [
        "# preprocessing the dataset\n",
        "\n",
        "def normalize_img(img):\n",
        "    norm_img = (img - img.min()) / (img.max() - img.min())\n",
        "    return norm_img\n",
        "\n",
        "def preprocess_image(img_path, horizontal_flip=False):\n",
        "  image = cv2.imread(img_path)\n",
        "  image = imutils.resize(image, height=HEIGHT)\n",
        "  # image = image[:, 21:149].astype(\"float\")\n",
        "  image = image.astype(\"float\")\n",
        "  image = normalize_img(image)\n",
        "\n",
        "  if horizontal_flip:\n",
        "    image = cv2.flip(image, 1)\n",
        "  return image\n",
        "\n",
        "def preprocess_depth_map(depth_map_path, horizontal_flip=False):\n",
        "  depth_map = cv2.imread(depth_map_path)\n",
        "  depth_map = cv2.cvtColor(depth_map, cv2.COLOR_BGR2GRAY)\n",
        "  depth_map = imutils.resize(depth_map, height=HEIGHT)\n",
        "  # depth_map = depth_map[:, 21:149].astype(\"float\")\n",
        "  depth_map = depth_map.astype(\"float\")\n",
        "  depth_map = normalize_img(depth_map)\n",
        "\n",
        "  if horizontal_flip:\n",
        "    depth_map = cv2.flip(depth_map, 1)\n",
        "\n",
        "  depth_map = np.reshape(depth_map, (depth_map.shape[0], depth_map.shape[1], 1))\n",
        "  return depth_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoGuninR2_L9"
      },
      "outputs": [],
      "source": [
        "# data generator\n",
        "\n",
        "from tensorflow.keras.utils import Sequence\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "  def __init__(self, list_IDs, labels, batch_size=16, dim=(128,128), n_channels=3, shuffle=True, pred=False):\n",
        "    self.dim = dim\n",
        "    self.batch_size = batch_size\n",
        "    self.labels = labels\n",
        "    self.list_IDs = list_IDs\n",
        "    self.n_channels = n_channels\n",
        "    self.shuffle = shuffle\n",
        "    self.pred = pred\n",
        "    self.on_epoch_end()\n",
        "\n",
        "  def __len__(self):\n",
        "    return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "    list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "    if self.pred:\n",
        "      X = self.__data_generation(list_IDs_temp)\n",
        "      return X\n",
        "    X, y = self.__data_generation(list_IDs_temp)\n",
        "    return X, y\n",
        "\n",
        "  def on_epoch_end(self):\n",
        "    self.indexes = np.arange(len(self.list_IDs))\n",
        "    if self.shuffle == True:\n",
        "      np.random.shuffle(self.indexes)\n",
        "\n",
        "  def __data_generation(self, list_IDs_temp):\n",
        "    X = np.empty((self.batch_size, self.dim[0], self.dim[1],self.n_channels))\n",
        "\n",
        "    if not self.pred:\n",
        "      y = np.empty((self.batch_size, self.dim[0], self.dim[1], 1))\n",
        "\n",
        "      for i, ID in enumerate(list_IDs_temp):\n",
        "        res = random.choice([True, False])\n",
        "        X[i,] = preprocess_image(ID, res)\n",
        "        y[i,] = preprocess_depth_map(self.labels[ID], res)\n",
        "      return X, y\n",
        "    else:\n",
        "      for i, ID in enumerate(list_IDs_temp):\n",
        "        res = random.choice([True, False])\n",
        "        X[i,] = preprocess_image(ID, res)\n",
        "      return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mA0wwrka3E_x"
      },
      "outputs": [],
      "source": [
        "partition, labels = load_train_paths(TRAIN_PATH)\n",
        "print(len(partition['train']), len(partition['validation']))\n",
        "training_generator = DataGenerator(list_IDs=partition['train'], labels=labels, batch_size=BATCH_SIZE, dim=(HEIGHT, WIDTH), n_channels=3, shuffle=True, pred=False)\n",
        "validation_generator = DataGenerator(list_IDs=partition['validation'], labels=labels, batch_size=BATCH_SIZE, dim=(HEIGHT, WIDTH), n_channels=3, shuffle=True, pred=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpqG8j8qB6gJ"
      },
      "source": [
        "#Utilities- standard functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7PecGFxerPD"
      },
      "outputs": [],
      "source": [
        "@keras.saving.register_keras_serializable(package=\"normalize_data\", name=\"normalize_data_format\")\n",
        "def normalize_data_format(value):\n",
        "    if value is None:\n",
        "        value = K.image_data_format()\n",
        "    data_format = value.lower()\n",
        "    if data_format not in {'channels_first', 'channels_last'}:\n",
        "        raise ValueError('The `data_format` argument must be one of '\n",
        "                         '\"channels_first\", \"channels_last\". Received: ' +\n",
        "                         str(value))\n",
        "    return data_format\n",
        "@keras.saving.register_keras_serializable(package=\"dwtdb4\", name=\"db4_dwt\")\n",
        "def db4_dwt(x):\n",
        "    \"\"\"\n",
        "    channels_last\n",
        "    :param x: [samples, widht, height, channels]\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    arr = list()\n",
        "    # for img in x:\n",
        "    coeffs2 = pywt.dwt2(x[:,:,:,:], 'db4')\n",
        "    LL, (LH, HL, HH) = coeffs2\n",
        "\n",
        "    return K.concatenate([LL], axis=-1)\n",
        "@keras.saving.register_keras_serializable(package=\"iwtdb4\", name=\"db4_iwt\")\n",
        "def db4_iwt(x):\n",
        "    \"\"\"\n",
        "    channels_last\n",
        "    :param x: [samples, widht, height, channels]\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    arr = numpy.array()\n",
        "    for img in x:\n",
        "        recon = pywt.idwt2(img, 'db4')\n",
        "        np.append(arr, recon)\n",
        "\n",
        "    return K.concatenate(arr, axis=-1)\n",
        "\n",
        "\n",
        "@keras.saving.register_keras_serializable(package=\"dwt\", name=\"dwt\")\n",
        "def dwt(x, data_format='channels_last'):\n",
        "\n",
        "    \"\"\"\n",
        "    DWT (Discrete Wavelet Transform) function implementation according to\n",
        "    \"Multi-level Wavelet Convolutional Neural Networks\"\n",
        "    by Pengju Liu, Hongzhi Zhang, Wei Lian, Wangmeng Zuo\n",
        "    https://arxiv.org/abs/1907.03128\n",
        "    \"\"\"\n",
        "\n",
        "    if data_format == 'channels_last':\n",
        "        # [all samplesL, width, height, neurons]\n",
        "        x1 = x[:, 0::2, 0::2, :]  # x(2i−1, 2j−1)\n",
        "        x2 = x[:, 1::2, 0::2, :]  # x(2i, 2j-1)\n",
        "        x3 = x[:, 0::2, 1::2, :]  # x(2i−1, 2j)\n",
        "        x4 = x[:, 1::2, 1::2, :]  # x(2i, 2j)\n",
        "\n",
        "    elif data_format == 'channels_first':\n",
        "        x1 = x[:, :, 0::2, 0::2]  # x(2i−1, 2j−1)\n",
        "        x2 = x[:, :, 1::2, 0::2]  # x(2i, 2j-1)\n",
        "        x3 = x[:, :, 0::2, 1::2]  # x(2i−1, 2j)\n",
        "        x4 = x[:, :, 1::2, 1::2]  # x(2i, 2j)\n",
        "\n",
        "    x_LL = x1 + x2 + x3 + x4\n",
        "    x_LH = -x1 - x3 + x2 + x4\n",
        "    x_HL = -x1 + x3 - x2 + x4\n",
        "    x_HH = x1 - x3 - x2 + x4\n",
        "\n",
        "    if data_format == 'channels_last':\n",
        "        return K.concatenate([x_LL, x_LH, x_HL, x_HH], axis=-1)\n",
        "    elif data_format == 'channels_first':\n",
        "        return K.concatenate([x_LL, x_LH, x_HL, x_HH], axis=1)\n",
        "\n",
        "\n",
        "@keras.saving.register_keras_serializable(package=\"iwt\", name=\"iwt\")\n",
        "def iwt(x, data_format='channels_last'):\n",
        "    \"\"\"\n",
        "    IWT (Inverse Wavelet Transfomr) function implementation according to\n",
        "    \"Multi-level Wavelet Convolutional Neural Networks\"\n",
        "    by Pengju Liu, Hongzhi Zhang, Wei Lian, Wangmeng Zuo\n",
        "    https://arxiv.org/abs/1907.03128\n",
        "    \"\"\"\n",
        "    if data_format == 'channels_last':\n",
        "\n",
        "        x_LL = x[:, :, :, 0:x.shape[3]//4]\n",
        "        x_LH = x[:, :, :, x.shape[3]//4:x.shape[3]//4*2]\n",
        "        x_HL = x[:, :, :, x.shape[3]//4*2:x.shape[3]//4*3]\n",
        "        x_HH = x[:, :, :, x.shape[3]//4*3:]\n",
        "\n",
        "        x1 = (x_LL - x_LH - x_HL + x_HH)/4\n",
        "        x2 = (x_LL - x_LH + x_HL - x_HH)/4\n",
        "        x3 = (x_LL + x_LH - x_HL - x_HH)/4\n",
        "        x4 = (x_LL + x_LH + x_HL + x_HH)/4\n",
        "\n",
        "        y1 = K.stack([x1, x3], axis=2)\n",
        "        y2 = K.stack([x2, x4], axis=2)\n",
        "        shape = K.shape(x)\n",
        "\n",
        "        return K.reshape(K.concatenate([y1, y2], axis=-1), K.stack([shape[0], shape[1]*2, shape[2]*2, shape[3]//4]))\n",
        "\n",
        "    elif data_format == 'channels_first':\n",
        "\n",
        "        raise RuntimeError('WIP, please use \"channels_last\" instead.')\n",
        "\n",
        "        x_LL = x[:, 0:x.shape[1]//4, :, :]\n",
        "        x_LH = x[:, x.shape[1]//4:x.shape[1]//4*2, :, :]\n",
        "        x_HL = x[:, x.shape[1]//4*2:x.shape[1]//4*3, :, :]\n",
        "        x_HH = x[:, x.shape[1]//4*3:, :, :]\n",
        "\n",
        "        x1 = (x_LL - x_LH - x_HL + x_HH)/4\n",
        "        x2 = (x_LL - x_LH + x_HL - x_HH)/4\n",
        "        x3 = (x_LL + x_LH - x_HL - x_HH)/4\n",
        "        x4 = (x_LL + x_LH + x_HL + x_HH)/4\n",
        "\n",
        "        y1 = K.stack([x1, x3], axis=3)\n",
        "        y2 = K.stack([x2, x4], axis=3)\n",
        "        shape = K.shape(x)\n",
        "        return K.reshape(K.concatenate([y1, y2], axis=1), K.stack([shape[0], shape[1]//4, shape[2]*2, shape[3]*2]))\n",
        "\n",
        "@keras.saving.register_keras_serializable(package=\"DWT_Pooling\")\n",
        "class DWT_Pooling(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Custom Layer performing DWT pooling operation described in :\n",
        "\n",
        "    \"Multi-level Wavelet Convolutional Neural Networks\"\n",
        "    by Pengju Liu, Hongzhi Zhang, Wei Lian, Wangmeng Zuo\n",
        "    https://arxiv.org/abs/1907.03128\n",
        "\n",
        "    # Arguments :\n",
        "        data_format (String): 'channels_first' or 'channels_last'\n",
        "\n",
        "    # Input shape :\n",
        "        If data_format='channels_last':\n",
        "            4D tensor of shape: (batch_size, rows, cols, channels)\n",
        "        If data_format='channels_first':\n",
        "            4D tensor of shape: (batch_size, channels, rows, cols)\n",
        "\n",
        "    # Output shape\n",
        "        If data_format='channels_last':\n",
        "            4D tensor of shape: (batch_size, rows/2, cols/2, channels*4)\n",
        "        If data_format='channels_first':\n",
        "            4D tensor of shape: (batch_size, channels*4, rows/2, cols/2)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_format=None,**kwargs):\n",
        "        super(DWT_Pooling, self).__init__(**kwargs)\n",
        "        self.data_format = normalize_data_format(data_format)\n",
        "        #self.data_format = keras.utils.conv_utils.normalize_data_format(data_format)\n",
        "\n",
        "    def get_config(self):  # added to save model\n",
        "         return {\"data_format\" : self.data_format}\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(DWT_Pooling, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        return dwt(x, self.data_format)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if self.data_format == 'channels_first':\n",
        "            return (input_shape[0], input_shape[1]*4, input_shape[2]//2, input_shape[3]//2)\n",
        "        elif self.data_format == 'channels_last':\n",
        "            return (input_shape[0], input_shape[1]//2, input_shape[2]//2, input_shape[3]*4)\n",
        "\n",
        "@keras.saving.register_keras_serializable(package=\"IWT_UpSampling\")\n",
        "class IWT_UpSampling(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Custom Layer performing IWT upsampling operation described in :\n",
        "\n",
        "    \"Multi-level Wavelet Convolutional Neural Networks\"\n",
        "    by Pengju Liu, Hongzhi Zhang, Wei Lian, Wangmeng Zuo\n",
        "    https://arxiv.org/abs/1907.03128\n",
        "\n",
        "    # Arguments :\n",
        "        data_format (String): 'channels_first' or 'channels_last'\n",
        "\n",
        "    # Input shape :\n",
        "        If data_format='channels_last':\n",
        "            4D tensor of shape: (batch_size, rows, cols, channels)\n",
        "        If data_format='channels_first':\n",
        "            4D tensor of shape: (batch_size, channels, rows, cols)\n",
        "\n",
        "    # Output shape\n",
        "        If data_format='channels_last':\n",
        "            4D tensor of shape: (batch_size, rows*2, cols*2, channels/4)\n",
        "        If data_format='channels_first':\n",
        "            4D tensor of shape: (batch_size, channels/4, rows*2, cols*2)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_format=None, **kwargs):\n",
        "        super(IWT_UpSampling, self).__init__(**kwargs)\n",
        "        #self.data_format = K.normalize_data_format(data_format)\n",
        "        self.data_format = normalize_data_format(data_format)\n",
        "\n",
        "    def get_config(self):  # added to save model\n",
        "         return {\"data_format\" : self.data_format}\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(IWT_UpSampling, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        return iwt(x, self.data_format)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if self.data_format == 'channels_first':\n",
        "            return ( input_shape[0], input_shape[1]//4, input_shape[2]*2, input_shape[3]*2 )\n",
        "        elif self.data_format == 'channels_last':\n",
        "            return ( input_shape[0], input_shape[1]*2, input_shape[2]*2, input_shape[3]//4 )\n",
        "\n",
        "@keras.saving.register_keras_serializable(package=\"DWT_Pooling_Db4\")\n",
        "class DWT_Pooling_Db4(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Custom Layer performing DWT pooling operation with db4 :\n",
        "    Author Sandip\n",
        "\n",
        "    # Output shape\n",
        "        If data_format='channels_last':\n",
        "            4D tensor of shape: (batch_size, rows/2, cols/2, channels*4)\n",
        "        If data_format='channels_first':\n",
        "            4D tensor of shape: (batch_size, channels*4, rows/2, cols/2)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(DWT_Pooling_Db4, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(DWT_Pooling_Db4, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        return db4_dwt(x)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[1]//2, input_shape[2]//2, input_shape[3]*4)\n",
        "\n",
        "@keras.saving.register_keras_serializable(package=\"IWT_UpSampling_Db4\")\n",
        "class IWT_UpSampling_Db4(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Custom Layer performing IWT upsampling operation described in :\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self , **kwargs):\n",
        "        super(IWT_UpSampling_Db4, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(IWT_UpSampling_Db4, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        return db4_iwt(x)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return ( input_shape[0], input_shape[1]*2, input_shape[2]*2, input_shape[3]//4 )\n",
        "#import tensorflow.keras as keras\n",
        "#from keras.layers import Conv2D, SeparableConv2D\n",
        "@keras.saving.register_keras_serializable(package=\"DepthwiseSeparableConv2D\")\n",
        "class DepthwiseSeparableConv2D(keras.layers.Layer):\n",
        "  def __init__(self, filters, kernel_size, kernel_initializer, padding, activation):\n",
        "    super(DepthwiseSeparableConv2D, self).__init__()\n",
        "    self.depthwise = DepthwiseConv2D(kernel_size = kernel_size, kernel_initializer = kernel_initializer, padding = padding, activation = activation)\n",
        "    self.pointwise = Conv2D(filters = filters, kernel_size = (1, 1), activation = activation)\n",
        "\n",
        "  def call(self, input_tensor):\n",
        "    x = self.depthwise(input_tensor)\n",
        "    return self.pointwise(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fp9Xzl_RwEpt"
      },
      "outputs": [],
      "source": [
        "# Residual\n",
        "def standard_unit(input_tensor, stage, nb_filter, kernel_size=(3, 3), batchnorm = True):\n",
        "    \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n",
        "    '''x = Conv2D(filters=nb_filter, kernel_size=(3,3), padding='same')(input_tensor)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = Conv2D(filters=nb_filter, kernel_size=(3,3), padding='same')(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    output = Conv2D(nb_filter, kernel_size, kernel_initializer = 'he_normal',padding=\"same\")(input_tensor)\n",
        "    output = LeakyReLU(alpha=0.2)(output)\n",
        "    output = Conv2D(nb_filter, kernel_size, kernel_initializer = 'he_normal',padding=\"same\")(output)\n",
        "    output = LeakyReLU(alpha=0.2)(output)\n",
        "    output = Conv2D(nb_filter, kernel_size, kernel_initializer = 'he_normal',padding=\"same\")(output)\n",
        "    x = LeakyReLU(alpha=0.2)(output)\n",
        "    x = BatchNormalization()(x)'''\n",
        "    # first layer\n",
        "    d = layers.Conv2D(nb_filter, kernel_size, kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
        "    #d = DepthwiseSeparableConv2D(filters=64, kernel_size=(3,3), padding=\"valid\", activation=\"relu\")(visible)\n",
        "    #d = DepthwiseSeparableConv2D(nb_filter, kernel_size, kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
        "    #d = SeparableConv2D(nb_filter, kernel_size,depthwise_initializer = 'he_normal', pointwise_initializer= 'he_normal',padding='same')(input_tensor)\n",
        "    #x = layers.Activation('relu')(d)\n",
        "    x = layers.BatchNormalization()(d)# axis-3\n",
        "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    # second layer\n",
        "    x = layers.Conv2D(nb_filter, kernel_size, kernel_initializer = 'he_normal', padding = 'same')(x)\n",
        "    #x = DepthwiseSeparableConv2D(nb_filter, kernel_size, kernel_initializer = 'he_normal', padding = 'same')(x)\n",
        "    #x = layers.Activation('relu')(x)\n",
        "    x = layers.BatchNormalization()(x)# axis-3\n",
        "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "    #x = layers.BatchNormalization()(x)# axis-3\n",
        "    # third layer\n",
        "    #x = layers.Conv2D(nb_filter, kernel_size, kernel_initializer = 'he_normal', padding = 'same')(x)\n",
        "    #x = layers.Activation('relu')(x)\n",
        "    #x = layers.BatchNormalization()(x)# axis-3\n",
        "    #x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "    #x = layers.BatchNormalization()(x)# axis-3\n",
        "    # fourth layer\n",
        "    x = layers.Conv2D(nb_filter, kernel_size, kernel_initializer = 'he_normal', padding = 'same')(x)\n",
        "    #x = DepthwiseSeparableConv2D(nb_filter, kernel_size, kernel_initializer = 'he_normal', padding = 'same')(x)\n",
        "    res_path = layers.add([d, x])\n",
        "    #x = layers.Activation('relu')(x)\n",
        "    x = layers.BatchNormalization()(res_path)# axis-3\n",
        "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "    #x = layers.BatchNormalization()(x)\n",
        "\n",
        "    #x += d\n",
        "    #shortcut = layers.Conv2D(nb_filter, kernel_size=(1, 1), padding='same')(input_tensor)\n",
        "    #if batchnorm:\n",
        "    #   shortcut = layers.BatchNormalization()(shortcut)\n",
        "    #res_path = layers.add([shortcut, x])\n",
        "    #x += shortcut\n",
        "    #x = layers.Activation('relu')(res_path)'''\n",
        "    return x\n",
        "\n",
        "def standard_unit1(input_tensor, stage, nb_filter, kernel_size = 3, batchnorm = True):\n",
        "    \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n",
        "    # first layer\n",
        "    x = layers.Conv2D(filters = nb_filter, kernel_size = (kernel_size, kernel_size),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
        "    x = layers.BatchNormalization()(x)# axis-3\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    # second layer\n",
        "    x = layers.Conv2D(filters = nb_filter, kernel_size = (kernel_size, kernel_size),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "def standard_unit(input_tensor, stage, nb_filter, kernel_size = 3, batchnorm = True):\n",
        "    \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n",
        "    # first layer\n",
        "    x = layers.Conv2D(filters = nb_filter, kernel_size = (kernel_size, kernel_size),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
        "    x = layers.BatchNormalization()(x)# axis-3\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    # second layer\n",
        "    x = layers.Conv2D(filters = nb_filter, kernel_size = (kernel_size, kernel_size),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "#changing and expirementing this\n",
        "# def standard_unit(input_tensor, stage, nb_filter, kernel_size = 3, batchnorm = True):\n",
        "#     # 1x1 convolution\n",
        "#     conv_1x1 = layers.Conv2D(filters=nb_filter, kernel_size=(1, 1),\n",
        "#                              kernel_initializer='he_normal', padding='same')(input_tensor)\n",
        "#     conv_1x1 = layers.BatchNormalization()(conv_1x1)\n",
        "#     conv_1x1 = layers.Activation('relu')(conv_1x1)\n",
        "\n",
        "#     # 3x3 convolution\n",
        "#     conv_3x3 = layers.Conv2D(filters=nb_filter, kernel_size=(3, 3),\n",
        "#                              kernel_initializer='he_normal', padding='same')(input_tensor)\n",
        "#     conv_3x3 = layers.BatchNormalization()(conv_3x3)\n",
        "#     conv_3x3 = layers.Activation('relu')(conv_3x3)\n",
        "\n",
        "#     # 5x5 convolution\n",
        "#     conv_5x5 = layers.Conv2D(filters=nb_filter, kernel_size=(5, 5),\n",
        "#                              kernel_initializer='he_normal', padding='same')(input_tensor)\n",
        "#     conv_5x5 = layers.BatchNormalization()(conv_5x5)\n",
        "#     conv_5x5 = layers.Activation('relu')(conv_5x5)\n",
        "\n",
        "#     # 3x3 max pooling followed by 1x1 convolution\n",
        "#     pool_proj = layers.MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(input_tensor)\n",
        "#     pool_proj = layers.Conv2D(filters=nb_filter, kernel_size=(1, 1),\n",
        "#                               kernel_initializer='he_normal', padding='same')(pool_proj)\n",
        "#     pool_proj = layers.BatchNormalization()(pool_proj)\n",
        "#     pool_proj = layers.Activation('relu')(pool_proj)\n",
        "\n",
        "#     # Concatenate all filters\n",
        "#     x = layers.concatenate([conv_1x1, conv_3x3, conv_5x5, pool_proj], axis=-1)\n",
        "#     return x\n",
        "# def standard_unit1(input_tensor, stage, nb_filter, kernel_size = 3, batchnorm = True):\n",
        "#     # 1x1 convolution\n",
        "#     conv_1x1 = layers.Conv2D(filters=nb_filter, kernel_size=(1, 1),\n",
        "#                              kernel_initializer='he_normal', padding='same')(input_tensor)\n",
        "#     conv_1x1 = layers.BatchNormalization()(conv_1x1)\n",
        "#     conv_1x1 = layers.Activation('relu')(conv_1x1)\n",
        "\n",
        "#     # 3x3 convolution\n",
        "#     conv_3x3 = layers.Conv2D(filters=nb_filter, kernel_size=(3, 3),\n",
        "#                              kernel_initializer='he_normal', padding='same')(input_tensor)\n",
        "#     conv_3x3 = layers.BatchNormalization()(conv_3x3)\n",
        "#     conv_3x3 = layers.Activation('relu')(conv_3x3)\n",
        "\n",
        "#     # 5x5 convolution\n",
        "#     conv_5x5 = layers.Conv2D(filters=nb_filter, kernel_size=(5, 5),\n",
        "#                              kernel_initializer='he_normal', padding='same')(input_tensor)\n",
        "#     conv_5x5 = layers.BatchNormalization()(conv_5x5)\n",
        "#     conv_5x5 = layers.Activation('relu')(conv_5x5)\n",
        "\n",
        "#     # 3x3 max pooling followed by 1x1 convolution\n",
        "#     pool_proj = layers.MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(input_tensor)\n",
        "#     pool_proj = layers.Conv2D(filters=nb_filter, kernel_size=(1, 1),\n",
        "#                               kernel_initializer='he_normal', padding='same')(pool_proj)\n",
        "#     pool_proj = layers.BatchNormalization()(pool_proj)\n",
        "#     pool_proj = layers.Activation('relu')(pool_proj)\n",
        "\n",
        "#     # Concatenate all filters\n",
        "#     x = layers.concatenate([conv_1x1, conv_3x3, conv_5x5, pool_proj], axis=-1)\n",
        "#     return x\n",
        "\n",
        "\n",
        "\n",
        "def gating_signal(input_tensor, nb_filter, batchnorm = True):\n",
        "    x = layers.Conv2D(filters = nb_filter, kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
        "    x = layers.BatchNormalization()(x)# axis-3\n",
        "    x = layers.Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "def attention_block(x, gs, inter_shape):\n",
        "    shape_x = K.int_shape(x)\n",
        "    shape_g = K.int_shape(gs)\n",
        "\n",
        "# Getting the x signal to the same shape as the gating signal\n",
        "    theta_x = layers.Conv2D(inter_shape, (2, 2), strides=(2, 2), padding='same')(x)  # 16\n",
        "    shape_theta_x = K.int_shape(theta_x)\n",
        "\n",
        "# Getting the gating signal to the same number of filters as the inter_shape\n",
        "    phi_g = layers.Conv2D(inter_shape, (1, 1), padding='same')(gs)\n",
        "    upsample_g = layers.Conv2DTranspose(inter_shape, (3, 3),\n",
        "                                 strides=(shape_theta_x[1] // shape_g[1], shape_theta_x[2] // shape_g[2]),\n",
        "                                 padding='same')(phi_g)  # 16\n",
        "\n",
        "    concat_xg = layers.add([upsample_g, theta_x])\n",
        "    act_xg = layers.Activation('relu')(concat_xg)\n",
        "    psi = layers.Conv2D(1, (1, 1), padding='same')(act_xg)\n",
        "    sigmoid_xg = layers.Activation('sigmoid')(psi)\n",
        "    shape_sigmoid = K.int_shape(sigmoid_xg)\n",
        "    upsample_psi = layers.UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]))(sigmoid_xg)  # 32\n",
        "\n",
        "    upsample_psi = repeat_elem(upsample_psi, shape_x[3])\n",
        "    y = layers.multiply([upsample_psi, x])\n",
        "\n",
        "    result = layers.Conv2D(shape_x[3], (1, 1), padding='same')(y)\n",
        "    result_bn = layers.BatchNormalization()(result)\n",
        "    return result_bn\n",
        "\n",
        "def repeat_elem(tensor, rep):\n",
        "    # lambda function to repeat Repeats the elements of a tensor along an axis\n",
        "    #by a factor of rep.\n",
        "    # If tensor has shape (None, 256,256,3), lambda will return a tensor of shape\n",
        "    #(None, 256,256,6), if specified axis=3 and rep=2.\n",
        "\n",
        "     return layers.Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3),\n",
        "                          arguments={'repnum': rep})(tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMFjJ6tS2GhU"
      },
      "outputs": [],
      "source": [
        "def downsampling_block(input_tensor, n_filters):\n",
        "  x = Conv2D(filters=n_filters, kernel_size=(3,3), padding='same')(input_tensor)\n",
        "  x = LeakyReLU(alpha=0.2)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  x = Conv2D(filters=n_filters, kernel_size=(3,3), padding='same')(x)\n",
        "  x = LeakyReLU(alpha=0.2)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  #shortcut = Conv2D(filters=n_filters, kernel_size=(1,1), padding='same')(input_tensor)\n",
        "  #shortcut = BatchNormalization()(shortcut)\n",
        "  #shortcut = LeakyReLU(alpha=0.2)(shortcut)\n",
        "  #x = add([shortcut, x])\n",
        "  return x\n",
        "\n",
        "def upsampling_block(input_tensor, n_filters, name, concat_with):\n",
        "  #x = UpSampling2D((2, 2), interpolation='bilinear', name=name)(input_tensor)\n",
        "  x = Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), name=name, padding='same')(input_tensor)\n",
        "  x = Conv2D(filters=n_filters, kernel_size=(3, 3), padding='same', name=name+\"_convA\")(x)\n",
        "  x = LeakyReLU(alpha=0.2)(x)\n",
        "  x = concatenate([x, concat_with], axis=3)\n",
        "\n",
        "  #shortcut = Conv2D(filters=n_filters, kernel_size=(1,1), padding='same')(x)\n",
        "  #shortcut = BatchNormalization()(shortcut)\n",
        "  #shortcut = LeakyReLU(alpha=0.2)(shortcut)\n",
        "\n",
        "  x = Conv2D(filters=n_filters, kernel_size=(3, 3), padding='same', name=name+\"_convB\")(x)\n",
        "  x = LeakyReLU(alpha=0.2)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  x = Conv2D(filters=n_filters, kernel_size=(3, 3), padding='same', name=name+\"_convC\")(x)\n",
        "  x = LeakyReLU(alpha=0.2)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  #x = add([shortcut, x])\n",
        "\n",
        "  return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUbacMztENYy"
      },
      "source": [
        "## Building the model\n",
        "\n",
        "1. The basic model is from U-Net.\n",
        "2. Addditive skip-connections are implemented in the downscaling block.\n",
        "3. Advanced models like DWT pooling Nested U-Net developed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlGz3c5EENY0"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DownscaleBlock(layers.Layer):\n",
        "    def __init__(\n",
        "        self, filters, kernel_size=(3, 3), padding=\"same\", strides=1, **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.convA = layers.Conv2D(filters, kernel_size, strides, padding)\n",
        "        self.convB = layers.Conv2D(filters, kernel_size, strides, padding)\n",
        "        self.reluA = layers.LeakyReLU(alpha=0.2)\n",
        "        self.reluB = layers.LeakyReLU(alpha=0.2)\n",
        "        self.bn2a = tf.keras.layers.BatchNormalization()\n",
        "        self.bn2b = tf.keras.layers.BatchNormalization()\n",
        "\n",
        "        self.pool = layers.MaxPool2D((2, 2), (2, 2))\n",
        "\n",
        "    def call(self, input_tensor):\n",
        "        d = self.convA(input_tensor)\n",
        "        x = self.bn2a(d)\n",
        "        x = self.reluA(x)\n",
        "\n",
        "        x = self.convB(x)\n",
        "        x = self.bn2b(x)\n",
        "        x = self.reluB(x)\n",
        "\n",
        "        x += d\n",
        "        p = self.pool(x)\n",
        "        return x, p\n",
        "\n",
        "\n",
        "class UpscaleBlock(layers.Layer):\n",
        "    def __init__(\n",
        "        self, filters, kernel_size=(3, 3), padding=\"same\", strides=1, **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.us = layers.UpSampling2D((2, 2))\n",
        "        self.convA = layers.Conv2D(filters, kernel_size, strides, padding)\n",
        "        self.convB = layers.Conv2D(filters, kernel_size, strides, padding)\n",
        "        self.reluA = layers.LeakyReLU(alpha=0.2)\n",
        "        self.reluB = layers.LeakyReLU(alpha=0.2)\n",
        "        self.bn2a = tf.keras.layers.BatchNormalization()\n",
        "        self.bn2b = tf.keras.layers.BatchNormalization()\n",
        "        self.conc = layers.Concatenate()\n",
        "\n",
        "    def call(self, x, skip):\n",
        "        x = self.us(x)\n",
        "        concat = self.conc([x, skip])\n",
        "        x = self.convA(concat)\n",
        "        x = self.bn2a(x)\n",
        "        x = self.reluA(x)\n",
        "\n",
        "        x = self.convB(x)\n",
        "        x = self.bn2b(x)\n",
        "        x = self.reluB(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class BottleNeckBlock(layers.Layer):\n",
        "    def __init__(\n",
        "        self, filters, kernel_size=(3, 3), padding=\"same\", strides=1, **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.convA = layers.Conv2D(filters, kernel_size, strides, padding)\n",
        "        self.convB = layers.Conv2D(filters, kernel_size, strides, padding)\n",
        "        self.reluA = layers.LeakyReLU(alpha=0.2)\n",
        "        self.reluB = layers.LeakyReLU(alpha=0.2)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.convA(x)\n",
        "        x = self.reluA(x)\n",
        "        x = self.convB(x)\n",
        "        x = self.reluB(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgGdr0rMENY4"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DepthEstimationModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.ssim_loss_weight = 1.0 #0.85\n",
        "        self.l1_loss_weight = 0.2\n",
        "        self.edge_loss_weight = 0.0\n",
        "        self.loss_metric = tf.keras.metrics.Mean(name=\"loss\")\n",
        "        f = [16, 32, 64, 128, 256]\n",
        "        self.downscale_blocks = [\n",
        "            DownscaleBlock(f[0]),\n",
        "            DownscaleBlock(f[1]),\n",
        "            DownscaleBlock(f[2]),\n",
        "            DownscaleBlock(f[3]),\n",
        "        ]\n",
        "        self.bottle_neck_block = BottleNeckBlock(f[4])\n",
        "        self.upscale_blocks = [\n",
        "            UpscaleBlock(f[3]),\n",
        "            UpscaleBlock(f[2]),\n",
        "            UpscaleBlock(f[1]),\n",
        "            UpscaleBlock(f[0]),\n",
        "        ]\n",
        "        self.conv_layer = layers.Conv2D(1, (1, 1), padding=\"same\", activation=\"tanh\")\n",
        "\n",
        "    def calculate_loss(self, target, pred):\n",
        "        # Edges\n",
        "        dy_true, dx_true = tf.image.image_gradients(target)\n",
        "        dy_pred, dx_pred = tf.image.image_gradients(pred)\n",
        "        weights_x = tf.exp(tf.reduce_mean(tf.abs(dx_true)))\n",
        "        weights_y = tf.exp(tf.reduce_mean(tf.abs(dy_true)))\n",
        "\n",
        "        # Depth smoothness\n",
        "        smoothness_x = dx_pred * weights_x\n",
        "        smoothness_y = dy_pred * weights_y\n",
        "\n",
        "        depth_smoothness_loss = tf.reduce_mean(abs(smoothness_x)) + tf.reduce_mean(\n",
        "            abs(smoothness_y)\n",
        "        )\n",
        "\n",
        "        # Structural similarity (SSIM) index\n",
        "        ssim_loss = tf.reduce_mean(\n",
        "            1\n",
        "            - tf.image.ssim(\n",
        "                target, pred, max_val=WIDTH, filter_size=7, k1=0.01 ** 2, k2=0.03 ** 2\n",
        "            )\n",
        "        )\n",
        "        # Point-wise depth\n",
        "        l1_loss = tf.reduce_mean(tf.abs(target - pred))\n",
        "\n",
        "        loss = (\n",
        "            (self.ssim_loss_weight * ssim_loss)\n",
        "            + (self.l1_loss_weight * l1_loss)\n",
        "            + (self.edge_loss_weight * depth_smoothness_loss))\n",
        "\n",
        "        return loss\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_metric]\n",
        "\n",
        "    def train_step(self, batch_data):\n",
        "        input, target = batch_data\n",
        "        with tf.GradientTape() as tape:\n",
        "            pred = self(input, training=True)\n",
        "            loss = self.calculate_loss(target, pred)\n",
        "\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        self.loss_metric.update_state(loss)\n",
        "        return {\n",
        "            \"loss\": self.loss_metric.result(),\n",
        "        }\n",
        "\n",
        "    def test_step(self, batch_data):\n",
        "        input, target = batch_data\n",
        "\n",
        "        pred = self(input, training=False)\n",
        "        loss = self.calculate_loss(target, pred)\n",
        "\n",
        "        self.loss_metric.update_state(loss)\n",
        "        return {\n",
        "            \"loss\": self.loss_metric.result(),\n",
        "        }\n",
        "\n",
        "    def call(self, x):\n",
        "        c1, p1 = self.downscale_blocks[0](x)\n",
        "        c2, p2 = self.downscale_blocks[1](p1)\n",
        "        c3, p3 = self.downscale_blocks[2](p2)\n",
        "        c4, p4 = self.downscale_blocks[3](p3)\n",
        "\n",
        "        bn = self.bottle_neck_block(p4)\n",
        "\n",
        "        u1 = self.upscale_blocks[0](bn, c4)\n",
        "        u2 = self.upscale_blocks[1](u1, c3)\n",
        "        u3 = self.upscale_blocks[2](u2, c2)\n",
        "        u4 = self.upscale_blocks[3](u3, c1)\n",
        "\n",
        "        return self.conv_layer(u4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6juyrJmmjfci"
      },
      "source": [
        "UNET- Version 2 ( Standard and Residual)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0FaGGxOtueY"
      },
      "outputs": [],
      "source": [
        "def UNet(img_rows, img_cols, color_type=3, num_class=1): # filter_num-16, dropout = 0.1\n",
        "    nb_filter = [32,64,128,256,512]\n",
        "    bn_axis = 3\n",
        "    img_input = Input(shape=(img_rows, img_cols, color_type), name='main_input')\n",
        "\n",
        "    conv1_1 = standard_unit1(img_input, stage='11', nb_filter=nb_filter[0])\n",
        "    pool1 = MaxPooling2D((2, 2), strides=(2, 2), name='pool1')(conv1_1)\n",
        "    #pool1 = MaxPooling2D(pool_size=(2, 2))(conv1_1)\n",
        "\n",
        "    conv2_1 = standard_unit1(pool1, stage='21', nb_filter=nb_filter[1])\n",
        "    pool2 = MaxPooling2D((2, 2), strides=(2, 2), name='pool2')(conv2_1)\n",
        "    #pool2 = MaxPooling2D(pool_size=(2, 2))(conv2_1)\n",
        "    conv3_1 = standard_unit1(pool2, stage='31', nb_filter=nb_filter[2])\n",
        "    pool3 = MaxPooling2D((2, 2), strides=(2, 2), name='pool3')(conv3_1)\n",
        "    #pool3 = MaxPooling2D(pool_size=(2, 2))(conv3_1)\n",
        "    conv4_1 = standard_unit1(pool3, stage='41', nb_filter=nb_filter[3])\n",
        "    pool4 = MaxPooling2D((2, 2), strides=(2, 2), name='pool4')(conv4_1)\n",
        "    #pool4 = MaxPooling2D(pool_size=(2, 2))(conv4_1)\n",
        "\n",
        "  # bottleneck\n",
        "    conv5_1 = standard_unit1(pool4, stage='51', nb_filter=nb_filter[4])\n",
        "\n",
        "    # Upsampling layers\n",
        "\n",
        "    up41 = layers.Conv2DTranspose(nb_filter[3], (2, 2), strides=(2, 2), name='up41', padding='same')(conv5_1)\n",
        "    #up41 = Conv2D(filters=nb_filter[3], kernel_size=(3, 3), padding='same', name=\"conv41\")(up41)\n",
        "    #up41 = LeakyReLU(alpha=0.2)(up41)\n",
        "    conv41 = layers.concatenate([up41, conv4_1], name='merge41', axis=bn_axis)\n",
        "    conv41 = standard_unit1(conv41, stage='41', nb_filter=nb_filter[3])\n",
        "\n",
        "    up31 = layers.Conv2DTranspose(nb_filter[2], (2, 2), strides=(2, 2), name='up31', padding='same')(conv41)\n",
        "    #up31 = Conv2D(filters=nb_filter[2], kernel_size=(3, 3), padding='same', name=\"conv31\")(up31)\n",
        "    #up31 = LeakyReLU(alpha=0.2)(up31)\n",
        "    conv31 = layers.concatenate([up31, conv3_1], name='merge31', axis=bn_axis)\n",
        "    conv31 = standard_unit1(conv31, stage='31', nb_filter=nb_filter[2])\n",
        "\n",
        "    up21 = layers.Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up21', padding='same')(conv31)\n",
        "    #up21 = Conv2D(filters=nb_filter[1], kernel_size=(3, 3), padding='same', name=\"conv21\")(up21)\n",
        "    #up21 = LeakyReLU(alpha=0.2)(up21)\n",
        "    conv21 = layers.concatenate([up21, conv2_1], name='merge21', axis=bn_axis)\n",
        "    conv21 = standard_unit1(conv21, stage='21', nb_filter=nb_filter[1])\n",
        "\n",
        "    up11 = layers.Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up11', padding='same')(conv21)\n",
        "    #up11 = Conv2D(filters=nb_filter[0], kernel_size=(3, 3), padding='same', name=\"conv11\")(up11)\n",
        "    #up11 = LeakyReLU(alpha=0.2)(up11)\n",
        "    conv11 = layers.concatenate([up11, conv1_1], name='merge11', axis=bn_axis)\n",
        "    conv11 = standard_unit1(conv11, stage='11', nb_filter=nb_filter[0])\n",
        "\n",
        "    # 1*1 convolutional layers\n",
        "    #conv_final = layers.Conv2D(num_class, (1, 1), activation='sigmoid', name='output_1', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv11)\n",
        "    conv_final = layers.Conv2D(num_class, kernel_size=(1,1), kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv11)\n",
        "    conv_final = layers.BatchNormalization(axis=3)(conv_final)\n",
        "    conv_final = layers.Activation('sigmoid', name='UNET')(conv_final)\n",
        "    #conv_final = Conv2D(filters=1, kernel_size=3, strides=(1,1), activation='sigmoid', padding='same', name='UNET')(conv11)\n",
        "    # Model\n",
        "    model = Model(img_input, [conv_final],name=\"U-Net\")\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyCewExkyzTk"
      },
      "source": [
        "Attention  + Unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6avQhq3iy4G_"
      },
      "outputs": [],
      "source": [
        "def AUNet(img_rows, img_cols, color_type=3, num_class=1): # filter_num-16, dropout = 0.1\n",
        "    nb_filter = [32,64,128,256,512]\n",
        "    bn_axis = 3\n",
        "    img_input = Input(shape=(img_rows, img_cols, color_type), name='main_input')\n",
        " # Downsampling layers\n",
        "    conv1_1 = standard_unit1(img_input, stage='11', nb_filter=nb_filter[0]) #128\n",
        "    pool1 = layers.MaxPooling2D((2, 2), strides=(2, 2), name='pool1')(conv1_1) #64\n",
        "\n",
        "    conv2_1 = standard_unit1(pool1, stage='21', nb_filter=nb_filter[1]) #64, 32\n",
        "    pool2 = MaxPooling2D((2, 2), strides=(2, 2), name='pool2')(conv2_1)\n",
        "\n",
        "    conv3_1 = standard_unit1(pool2, stage='31', nb_filter=nb_filter[2]) #32,16\n",
        "    pool3 = MaxPooling2D((2, 2), strides=(2, 2), name='pool3')(conv3_1)\n",
        "\n",
        "    conv4_1 = standard_unit1(pool3, stage='41', nb_filter=nb_filter[3])#16,8\n",
        "    pool4 = MaxPooling2D((2, 2), strides=(2, 2), name='pool4')(conv4_1)\n",
        "\n",
        "    conv5_1 = standard_unit1(pool4, stage='51', nb_filter=nb_filter[4]) #conv_8\n",
        "\n",
        "    # Upsampling layers\n",
        "    gs41=gating_signal(conv5_1, nb_filter[3]) #16\n",
        "    a41= attention_block(conv4_1, gs41, nb_filter[3])\n",
        "    up41 = layers.Conv2DTranspose(nb_filter[3], (2, 2), strides=(2, 2), name='up41', padding='same')(conv5_1)\n",
        "    conv41 = layers.concatenate([up41, a41], name='merge41', axis=bn_axis)\n",
        "    uconv41 = standard_unit1(conv41, stage='41', nb_filter=nb_filter[3])\n",
        "\n",
        "    gs31=gating_signal(conv4_1, nb_filter[2]) #32\n",
        "    a31= attention_block(conv3_1, gs31, nb_filter[2])\n",
        "    up31 = layers.Conv2DTranspose(nb_filter[2], (2, 2), strides=(2, 2), name='up31', padding='same')(uconv41)\n",
        "    conv31 = layers.concatenate([up31, a31], name='merge31', axis=bn_axis)\n",
        "    uconv31 = standard_unit1(conv31, stage='31', nb_filter=nb_filter[2])\n",
        "\n",
        "    gs21=gating_signal(conv3_1, nb_filter[1]) #64\n",
        "    a21= attention_block(conv2_1, gs21, nb_filter[1])\n",
        "    up21 = layers.Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up21', padding='same')(uconv31)\n",
        "    conv21 = layers.concatenate([up21, a21], name='merge21', axis=bn_axis)\n",
        "    uconv21 = standard_unit1(conv21, stage='21', nb_filter=nb_filter[1])\n",
        "\n",
        "    gs11=gating_signal(conv2_1, nb_filter[0]) #128\n",
        "    a11= attention_block(conv1_1, gs11, nb_filter[0])\n",
        "    up11 = layers.Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up11', padding='same')(uconv21)\n",
        "    conv11 = layers.concatenate([up11, a11], name='merge11', axis=bn_axis)\n",
        "    uconv11 = standard_unit1(conv11, stage='11', nb_filter=nb_filter[0])\n",
        "\n",
        "    # 1*1 convolutional layers\n",
        "    #conv_final = layers.Conv2D(num_class, (1, 1), activation='sigmoid', name='output_1', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv11)\n",
        "    conv_final = layers.Conv2D(num_class, kernel_size=(1,1), kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(uconv11)\n",
        "    conv_final = layers.BatchNormalization(axis=3)(conv_final)\n",
        "    conv_final = layers.Activation('sigmoid', name='AUNET')(conv_final)\n",
        "\n",
        "    # Model\n",
        "    model = Model(img_input, [conv_final], name=\"Attn-UNet\")\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV7aVkMJYRJ8"
      },
      "source": [
        "#Discrete Wavelet (DWT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4htSS0CgkW0"
      },
      "outputs": [],
      "source": [
        "# from https://github.com/AureliePeng/Keras-WaveletTransform\n",
        "\n",
        "def unetWavelet(height, width, depth):\n",
        "    input_size=(height, width, depth)\n",
        "    n_filters=64\n",
        "    def down_block(input_layer, filters, kernel_size=(3, 3), activation=\"relu\"):\n",
        "        #output = Conv2D(filters, kernel_size, kernel_initializer = 'he_normal',padding=\"same\", activation=activation)(input_layer)\n",
        "        output = Conv2D(filters, kernel_size, kernel_initializer = 'he_normal',padding=\"same\")(input_layer)\n",
        "        output = LeakyReLU(alpha=0.2)(output)\n",
        "        output = Conv2D(filters, kernel_size, kernel_initializer = 'he_normal',padding=\"same\")(output)\n",
        "        output = LeakyReLU(alpha=0.2)(output)\n",
        "        output = Conv2D(filters, kernel_size, kernel_initializer = 'he_normal',padding=\"same\")(output)\n",
        "        output = LeakyReLU(alpha=0.2)(output)\n",
        "        return output, DWT_Pooling()(output)\n",
        "\n",
        "\n",
        "    def up_block(input_layer, residual_layer, filters, kernel_size=(3, 3), activation=\"relu\"):\n",
        "        output = IWT_UpSampling()(input_layer)\n",
        "        output = Add()([residual_layer, output])\n",
        "        output = Conv2D(filters, kernel_size, kernel_initializer = 'he_normal', padding=\"same\")(output)\n",
        "        output = LeakyReLU(alpha=0.2)(output)\n",
        "        output = Conv2D(filters, kernel_size, kernel_initializer = 'he_normal', padding=\"same\")(output)\n",
        "        output = LeakyReLU(alpha=0.2)(output)\n",
        "        output = Conv2D(filters * 2, kernel_size, kernel_initializer = 'he_normal', padding=\"same\")(output)\n",
        "        output = LeakyReLU(alpha=0.2)(output)\n",
        "        return output\n",
        "\n",
        "    inputs = Input(shape=input_size)\n",
        "\n",
        "    down1, pool1 = down_block(inputs, n_filters * 1)\n",
        "    down2, pool2 = down_block(pool1, n_filters * 2)\n",
        "    down3, pool3 = down_block(pool2, n_filters * 4)\n",
        "    down4, pool4 = down_block(pool3, n_filters * 8)\n",
        "\n",
        "    down5 = Conv2D(filters=n_filters * 16, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")(pool4)\n",
        "    down5 = Conv2D(filters=n_filters * 16, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")(down5)\n",
        "    down5 = Conv2D(filters=n_filters * 32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")(down5)\n",
        "\n",
        "    up = up_block(down5, down4, n_filters * 8)\n",
        "    up = up_block(up, down3, n_filters * 4)\n",
        "    up = up_block(up, down2, n_filters * 2)\n",
        "    up = up_block(up, down1, n_filters * 1)\n",
        "\n",
        "    #output = Conv2D(filters=input_size[2], kernel_size=(1, 1), padding=\"same\")(up)\n",
        "    outputs = Conv2D(1, (1, 1), padding=\"same\")(up)\n",
        "    model = Model(inputs=[inputs], outputs=[outputs],name=\"DWT-UNet1\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wbuPzZjAnT5"
      },
      "source": [
        "DWTUnet _ version 2 (Standard & Residual)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGbL-HI9YWAd"
      },
      "outputs": [],
      "source": [
        "def Udwt(img_rows, img_cols, color_type=3, num_class=1): # filter_num-16, dropout = 0.1\n",
        "    nb_filter = [32,64,128,256,512]\n",
        "    bn_axis = 3\n",
        "    img_input = Input(shape=(img_rows, img_cols, color_type), name='main_input')\n",
        "\n",
        "    conv1_1 = standard_unit1(img_input, stage='11', nb_filter=nb_filter[0])\n",
        "    pool1 = DWT_Pooling()(conv1_1)\n",
        "\n",
        "    conv2_1 = standard_unit1(pool1, stage='21', nb_filter=nb_filter[1])\n",
        "    pool2 = DWT_Pooling()(conv2_1)\n",
        "\n",
        "    conv3_1 = standard_unit1(pool2, stage='31', nb_filter=nb_filter[2])\n",
        "    pool3 = DWT_Pooling()(conv3_1)\n",
        "\n",
        "    conv4_1 = standard_unit1(pool3, stage='41', nb_filter=nb_filter[3])\n",
        "    pool4 = DWT_Pooling()(conv4_1)\n",
        "    ##############changing to stdunit1\n",
        "    conv5_1 = standard_unit(pool4, stage='51', nb_filter=nb_filter[4])\n",
        "\n",
        "    up41 = IWT_UpSampling()(conv5_1)\n",
        "    up41 = layers.Conv2D(filters = nb_filter[3], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up41)\n",
        "    conv41 = Add()([up41, conv4_1])\n",
        "    conv41 = standard_unit1(conv41, stage='12', nb_filter=nb_filter[3])\n",
        "\n",
        "    up31 = IWT_UpSampling()(conv41)\n",
        "    up31 = layers.Conv2D(filters = nb_filter[2], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up31)\n",
        "    conv31 = Add()([up31, conv3_1])\n",
        "    conv31 = standard_unit1(conv31, stage='12', nb_filter=nb_filter[2])\n",
        "\n",
        "    up21 = IWT_UpSampling()(conv31)\n",
        "    up21 = layers.Conv2D(filters = nb_filter[1], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up21)\n",
        "    conv21 = Add()([up21, conv2_1])\n",
        "    conv21 = standard_unit1(conv21, stage='12', nb_filter=nb_filter[1])\n",
        "\n",
        "    up11 = IWT_UpSampling()(conv21)\n",
        "    up11 = layers.Conv2D(filters = nb_filter[0], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up11)\n",
        "    conv11 = Add()([up11, conv1_1])\n",
        "    conv11 = standard_unit1(conv11, stage='12', nb_filter=nb_filter[0])\n",
        "\n",
        "    # 1*1 convolutional layers\n",
        "    #conv_final = layers.Conv2D(num_class, (1, 1), activation='sigmoid', name='output_1', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv11)\n",
        "    conv_final = layers.Conv2D(num_class, kernel_size=(1,1), kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv11)\n",
        "    conv_final = layers.BatchNormalization(axis=3)(conv_final)\n",
        "    conv_final = layers.Activation('sigmoid', name='UDWT')(conv_final)\n",
        "\n",
        "    # Model\n",
        "    model = Model(img_input, [conv_final],name=\"DWT-UNet2\")\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcn-zg6MLSz_"
      },
      "source": [
        "# changing dwtunet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YoUGQy7LNMk"
      },
      "outputs": [],
      "source": [
        "def Udwt(img_rows, img_cols, color_type=3, num_class=1): # filter_num-16, dropout = 0.1\n",
        "    nb_filter = [16,32,64,128,256,512,1024]\n",
        "    bn_axis = 3\n",
        "    img_input = Input(shape=(img_rows, img_cols, color_type), name='main_input')\n",
        "\n",
        "    conv1_1 = standard_unit1(img_input, stage='11', nb_filter=nb_filter[0])\n",
        "    pool1 = DWT_Pooling()(conv1_1)\n",
        "\n",
        "    conv2_1 = standard_unit1(pool1, stage='21', nb_filter=nb_filter[1])\n",
        "    pool2 = DWT_Pooling()(conv2_1)\n",
        "\n",
        "    conv3_1 = standard_unit1(pool2, stage='31', nb_filter=nb_filter[2])\n",
        "    pool3 = DWT_Pooling()(conv3_1)\n",
        "\n",
        "    conv4_1 = standard_unit1(pool3, stage='41', nb_filter=nb_filter[3])\n",
        "    pool4 = DWT_Pooling()(conv4_1)\n",
        "    ##############changing to stdunit1\n",
        "    conv5_1 = standard_unit(pool4, stage='51', nb_filter=nb_filter[4])\n",
        "\n",
        "    up41 = IWT_UpSampling()(conv5_1)\n",
        "    up41 = layers.Conv2D(filters = nb_filter[5], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up41)\n",
        "    print(up41.shape)\n",
        "    print(conv4_1.shape)\n",
        "    conv41 = Add()([up41, conv4_1])\n",
        "    conv41 = standard_unit1(conv41, stage='12', nb_filter=nb_filter[3])\n",
        "\n",
        "    up31 = IWT_UpSampling()(conv41)\n",
        "    up31 = layers.Conv2D(filters = nb_filter[4], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up31)\n",
        "    print(up31.shape)\n",
        "    print(conv3_1.shape)\n",
        "    conv31 = Add()([up31, conv3_1])\n",
        "    conv31 = standard_unit1(conv31, stage='12', nb_filter=nb_filter[2])\n",
        "\n",
        "    up21 = IWT_UpSampling()(conv31)\n",
        "    up21 = layers.Conv2D(filters = nb_filter[3], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up21)\n",
        "    print(up21.shape)\n",
        "    print(conv2_1.shape)\n",
        "    conv21 = Add()([up21, conv2_1])\n",
        "    conv21 = standard_unit1(conv21, stage='12', nb_filter=nb_filter[1])\n",
        "\n",
        "    up11 = IWT_UpSampling()(conv21)\n",
        "    up11 = layers.Conv2D(filters = nb_filter[2], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up11)\n",
        "    print(up11.shape)\n",
        "    print(conv1_1.shape)\n",
        "    conv11 = Add()([up11, conv1_1])\n",
        "    conv11 = standard_unit1(conv11, stage='12', nb_filter=nb_filter[0])\n",
        "\n",
        "    # 1*1 convolutional layers\n",
        "    #conv_final = layers.Conv2D(num_class, (1, 1), activation='sigmoid', name='output_1', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv11)\n",
        "    conv_final = layers.Conv2D(num_class, kernel_size=(1,1), kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv11)\n",
        "    conv_final = layers.BatchNormalization(axis=3)(conv_final)\n",
        "    conv_final = layers.Activation('sigmoid', name='UDWT')(conv_final)\n",
        "\n",
        "    # Model\n",
        "    model = Model(img_input, [conv_final],name=\"DWT-UNet2\")\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKQjC3BLLKHg"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = Udwt(HEIGHT, WIDTH, color_type=3, num_class=1)\n",
        "\n",
        "model.compile(optimizer=opt, loss=depth_loss, metrics=[depth_acc])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_5OsA1ZkM23"
      },
      "source": [
        "Attension + DWT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIo2AE5aieaC"
      },
      "outputs": [],
      "source": [
        "def AUdwt(img_rows, img_cols, color_type=3, num_class=1): # filter_num-16, dropout = 0.1\n",
        "    nb_filter = [32,64,128,256,512]\n",
        "    bn_axis = 3\n",
        "    img_input = Input(shape=(img_rows, img_cols, color_type), name='main_input')\n",
        " # Downsampling layers\n",
        "    conv1_1 = standard_unit1(img_input, stage='11', nb_filter=nb_filter[0])\n",
        "    pool1 = DWT_Pooling()(conv1_1)\n",
        "\n",
        "    conv2_1 = standard_unit1(pool1, stage='21', nb_filter=nb_filter[1])\n",
        "    pool2 = DWT_Pooling()(conv2_1)\n",
        "\n",
        "    conv3_1 = standard_unit1(pool2, stage='31', nb_filter=nb_filter[2])\n",
        "    pool3 = DWT_Pooling()(conv3_1)\n",
        "\n",
        "    conv4_1 = standard_unit1(pool3, stage='41', nb_filter=nb_filter[3])\n",
        "    pool4 = DWT_Pooling()(conv4_1)\n",
        "\n",
        "    conv5_1 = standard_unit(pool4, stage='51', nb_filter=nb_filter[4])\n",
        "\n",
        "    # Upsampling layers\n",
        "    gs41=gating_signal(conv5_1, nb_filter[3]) #16\n",
        "    a41= attention_block(conv4_1, gs41, nb_filter[3])\n",
        "    up41 = IWT_UpSampling()(conv5_1)\n",
        "    up41 = layers.Conv2D(filters = nb_filter[3], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up41)\n",
        "    conv41 = Add()([up41, a41])\n",
        "    conv41 = standard_unit1(conv41, stage='41', nb_filter=nb_filter[3])\n",
        "\n",
        "    gs31=gating_signal(conv4_1, nb_filter[2]) #32\n",
        "    a31= attention_block(conv3_1, gs31, nb_filter[2])\n",
        "    up31 = IWT_UpSampling()(conv41)\n",
        "    up31 = layers.Conv2D(filters = nb_filter[2], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up31)\n",
        "    conv31 = Add()([up31, a31])\n",
        "    conv31 = standard_unit1(conv31, stage='31', nb_filter=nb_filter[2])\n",
        "\n",
        "    gs21=gating_signal(conv3_1, nb_filter[1]) #64\n",
        "    a21= attention_block(conv2_1, gs21, nb_filter[1])\n",
        "    up21 = IWT_UpSampling()(conv31)\n",
        "    up21 = layers.Conv2D(filters = nb_filter[1], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up21)\n",
        "    conv21 = Add()([up21, a21])\n",
        "    conv21 = standard_unit1(conv21, stage='21', nb_filter=nb_filter[1])\n",
        "\n",
        "    gs11=gating_signal(conv2_1, nb_filter[0]) #128\n",
        "    a11= attention_block(conv1_1, gs11, nb_filter[0])\n",
        "    up11 = IWT_UpSampling()(conv21)\n",
        "    up11 = layers.Conv2D(filters = nb_filter[0], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up11)\n",
        "    conv11 = Add()([up11, a11])\n",
        "    conv11 = standard_unit1(conv11, stage='11', nb_filter=nb_filter[0])\n",
        "\n",
        "    # 1*1 convolutional layers\n",
        "    #conv_final = layers.Conv2D(num_class, (1, 1), activation='sigmoid', name='output_1', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv11)\n",
        "    conv_final = layers.Conv2D(num_class, kernel_size=(1,1), kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv11)\n",
        "    conv_final = layers.BatchNormalization(axis=3)(conv_final)\n",
        "    conv_final = layers.Activation('sigmoid', name='UNET')(conv_final)\n",
        "\n",
        "    # Model\n",
        "    model = Model(img_input, [conv_final],name=\"AttnDWT-UNet\")\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1RV-QS2l4fl"
      },
      "source": [
        "#*UNET*++ (Standard & Residual)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2ynMwICly6G"
      },
      "outputs": [],
      "source": [
        "def Nest_Net(img_rows, img_cols, color_type=3, num_class=1, deep_supervision=False):\n",
        "    #added 16\n",
        "    nb_filter = [16,32,64,128,256,512]\n",
        "\n",
        "    bn_axis = 3\n",
        "    img_input = Input(shape=(img_rows, img_cols, color_type), name='main_input')\n",
        "\n",
        "    conv1_1 = standard_unit(img_input, stage='11', nb_filter=nb_filter[0])\n",
        "    pool1 = layers.MaxPooling2D((2, 2), strides=(2, 2), name='pool1')(conv1_1)\n",
        "\n",
        "    conv2_1 = standard_unit(pool1, stage='21', nb_filter=nb_filter[1])\n",
        "    pool2 = MaxPooling2D((2, 2), strides=(2, 2), name='pool2')(conv2_1)\n",
        "\n",
        "    up1_2 = layers.Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up12', padding='same')(conv2_1)\n",
        "    conv1_2 = layers.concatenate([up1_2, conv1_1], name='merge12', axis=bn_axis)\n",
        "    conv1_2 = standard_unit(conv1_2, stage='12', nb_filter=nb_filter[0])\n",
        "\n",
        "    conv3_1 = standard_unit(pool2, stage='31', nb_filter=nb_filter[2])\n",
        "    pool3 = MaxPooling2D((2, 2), strides=(2, 2), name='pool3')(conv3_1)\n",
        "\n",
        "    up2_2 = layers.Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up22', padding='same')(conv3_1)\n",
        "    conv2_2 = layers.concatenate([up2_2, conv2_1], name='merge22', axis=bn_axis)\n",
        "    conv2_2 = standard_unit(conv2_2, stage='22', nb_filter=nb_filter[1])\n",
        "\n",
        "    up1_3 = layers.Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up13', padding='same')(conv2_2)\n",
        "    conv1_3 = layers.concatenate([up1_3, conv1_1, conv1_2], name='merge13', axis=bn_axis)\n",
        "    conv1_3 = standard_unit(conv1_3, stage='13', nb_filter=nb_filter[0])\n",
        "\n",
        "    conv4_1 = standard_unit(pool3, stage='41', nb_filter=nb_filter[3])\n",
        "    pool4 = MaxPooling2D((2, 2), strides=(2, 2), name='pool4')(conv4_1)\n",
        "\n",
        "    up3_2 = layers.Conv2DTranspose(nb_filter[2], (2, 2), strides=(2, 2), name='up32', padding='same')(conv4_1)\n",
        "    conv3_2 = layers.concatenate([up3_2, conv3_1], name='merge32', axis=bn_axis)\n",
        "    conv3_2 = standard_unit(conv3_2, stage='32', nb_filter=nb_filter[2])\n",
        "\n",
        "    up2_3 = layers.Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up23', padding='same')(conv3_2)\n",
        "    conv2_3 = layers.concatenate([up2_3, conv2_1, conv2_2], name='merge23', axis=bn_axis)\n",
        "    conv2_3 = standard_unit(conv2_3, stage='23', nb_filter=nb_filter[1])\n",
        "\n",
        "    up1_4 = layers.Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up14', padding='same')(conv2_3)\n",
        "    conv1_4 = layers.concatenate([up1_4, conv1_1, conv1_2, conv1_3], name='merge14', axis=bn_axis)\n",
        "    conv1_4 = standard_unit(conv1_4, stage='14', nb_filter=nb_filter[0])\n",
        "\n",
        "    conv5_1 = standard_unit(pool4, stage='51', nb_filter=nb_filter[4])\n",
        "\n",
        "    up4_2 = layers.Conv2DTranspose(nb_filter[3], (2, 2), strides=(2, 2), name='up42', padding='same')(conv5_1)\n",
        "    conv4_2 = layers.concatenate([up4_2, conv4_1], name='merge42', axis=bn_axis)\n",
        "    conv4_2 = standard_unit(conv4_2, stage='42', nb_filter=nb_filter[3])\n",
        "\n",
        "    up3_3 = layers.Conv2DTranspose(nb_filter[2], (2, 2), strides=(2, 2), name='up33', padding='same')(conv4_2)\n",
        "    conv3_3 = layers.concatenate([up3_3, conv3_1, conv3_2], name='merge33', axis=bn_axis)\n",
        "    conv3_3 = standard_unit(conv3_3, stage='33', nb_filter=nb_filter[2])\n",
        "\n",
        "    up2_4 = layers.Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up24', padding='same')(conv3_3)\n",
        "    conv2_4 = layers.concatenate([up2_4, conv2_1, conv2_2, conv2_3], name='merge24', axis=bn_axis)\n",
        "    conv2_4 = standard_unit(conv2_4, stage='24', nb_filter=nb_filter[1])\n",
        "\n",
        "    up1_5 = layers.Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up15', padding='same')(conv2_4)\n",
        "    conv1_5 = layers.concatenate([up1_5, conv1_1, conv1_2, conv1_3, conv1_4], name='merge15', axis=bn_axis)\n",
        "    conv1_5 = standard_unit(conv1_5, stage='15', nb_filter=nb_filter[0])\n",
        "\n",
        "    #nestnet_output_1 = layers.Conv2D(num_class, (1, 1), activation='sigmoid', name='output_1', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_2)\n",
        "    #nestnet_output_2 = layers.Conv2D(num_class, (1, 1), activation='sigmoid', name='output_2', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_3)\n",
        "    #nestnet_output_3 = layers.Conv2D(num_class, (1, 1), activation='sigmoid', name='output_3', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_4)\n",
        "    nestnet_output_4 = layers.Conv2D(num_class, (1, 1), activation='sigmoid', name='UNETP', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_5)\n",
        "\n",
        "    if deep_supervision:\n",
        "        model = Model(img_input, [nestnet_output_1,nestnet_output_2,nestnet_output_3,nestnet_output_4])\n",
        "    else:\n",
        "        model = Model(img_input, [nestnet_output_4], name=\"UNetp\")\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tf0j5M_le8lK"
      },
      "source": [
        "## nestnet pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7gH7z3mOoS8"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-model-optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lx-G2SZEe8LZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, BatchNormalization, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
        "def Nest_Net(img_rows, img_cols, color_type=3, num_class=1, deep_supervision=False):\n",
        "\n",
        "    nb_filter = [32,64,128,256,512]\n",
        "\n",
        "    bn_axis = 3\n",
        "    img_input = Input(shape=(img_rows, img_cols, color_type), name='main_input')\n",
        "\n",
        "    conv1_1 = standard_unit(img_input, stage='11', nb_filter=nb_filter[0])\n",
        "    pool1 = layers.MaxPooling2D((2, 2), strides=(2, 2), name='pool1')(conv1_1)\n",
        "\n",
        "    conv2_1 = standard_unit(pool1, stage='21', nb_filter=nb_filter[1])\n",
        "    pool2 = MaxPooling2D((2, 2), strides=(2, 2), name='pool2')(conv2_1)\n",
        "\n",
        "    up1_2 = layers.Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up12', padding='same')(conv2_1)\n",
        "    conv1_2 = layers.concatenate([up1_2, conv1_1], name='merge12', axis=bn_axis)\n",
        "    conv1_2 = standard_unit(conv1_2, stage='12', nb_filter=nb_filter[0])\n",
        "\n",
        "    conv3_1 = standard_unit(pool2, stage='31', nb_filter=nb_filter[2])\n",
        "    pool3 = MaxPooling2D((2, 2), strides=(2, 2), name='pool3')(conv3_1)\n",
        "\n",
        "    up2_2 = layers.Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up22', padding='same')(conv3_1)\n",
        "    conv2_2 = layers.concatenate([up2_2, conv2_1], name='merge22', axis=bn_axis)\n",
        "    conv2_2 = standard_unit(conv2_2, stage='22', nb_filter=nb_filter[1])\n",
        "\n",
        "    up1_3 = layers.Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up13', padding='same')(conv2_2)\n",
        "    conv1_3 = layers.concatenate([up1_3, conv1_1, conv1_2], name='merge13', axis=bn_axis)\n",
        "    conv1_3 = standard_unit(conv1_3, stage='13', nb_filter=nb_filter[0])\n",
        "\n",
        "    conv4_1 = standard_unit(pool3, stage='41', nb_filter=nb_filter[3])\n",
        "    pool4 = MaxPooling2D((2, 2), strides=(2, 2), name='pool4')(conv4_1)\n",
        "\n",
        "    up3_2 = layers.Conv2DTranspose(nb_filter[2], (2, 2), strides=(2, 2), name='up32', padding='same')(conv4_1)\n",
        "    conv3_2 = layers.concatenate([up3_2, conv3_1], name='merge32', axis=bn_axis)\n",
        "    conv3_2 = standard_unit(conv3_2, stage='32', nb_filter=nb_filter[2])\n",
        "\n",
        "    up2_3 = layers.Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up23', padding='same')(conv3_2)\n",
        "    conv2_3 = layers.concatenate([up2_3, conv2_1, conv2_2], name='merge23', axis=bn_axis)\n",
        "    conv2_3 = standard_unit(conv2_3, stage='23', nb_filter=nb_filter[1])\n",
        "\n",
        "    up1_4 = layers.Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up14', padding='same')(conv2_3)\n",
        "    conv1_4 = layers.concatenate([up1_4, conv1_1, conv1_2, conv1_3], name='merge14', axis=bn_axis)\n",
        "    conv1_4 = standard_unit(conv1_4, stage='14', nb_filter=nb_filter[0])\n",
        "\n",
        "    conv5_1 = standard_unit(pool4, stage='51', nb_filter=nb_filter[4])\n",
        "\n",
        "    up4_2 = layers.Conv2DTranspose(nb_filter[3], (2, 2), strides=(2, 2), name='up42', padding='same')(conv5_1)\n",
        "    conv4_2 = layers.concatenate([up4_2, conv4_1], name='merge42', axis=bn_axis)\n",
        "    conv4_2 = standard_unit(conv4_2, stage='42', nb_filter=nb_filter[3])\n",
        "\n",
        "    up3_3 = layers.Conv2DTranspose(nb_filter[2], (2, 2), strides=(2, 2), name='up33', padding='same')(conv4_2)\n",
        "    conv3_3 = layers.concatenate([up3_3, conv3_1, conv3_2], name='merge33', axis=bn_axis)\n",
        "    conv3_3 = standard_unit(conv3_3, stage='33', nb_filter=nb_filter[2])\n",
        "\n",
        "    up2_4 = layers.Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up24', padding='same')(conv3_3)\n",
        "    conv2_4 = layers.concatenate([up2_4, conv2_1, conv2_2, conv2_3], name='merge24', axis=bn_axis)\n",
        "    conv2_4 = standard_unit(conv2_4, stage='24', nb_filter=nb_filter[1])\n",
        "\n",
        "    up1_5 = layers.Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up15', padding='same')(conv2_4)\n",
        "    conv1_5 = layers.concatenate([up1_5, conv1_1, conv1_2, conv1_3, conv1_4], name='merge15', axis=bn_axis)\n",
        "    conv1_5 = standard_unit(conv1_5, stage='15', nb_filter=nb_filter[0])\n",
        "\n",
        "    #nestnet_output_1 = layers.Conv2D(num_class, (1, 1), activation='sigmoid', name='output_1', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_2)\n",
        "    #nestnet_output_2 = layers.Conv2D(num_class, (1, 1), activation='sigmoid', name='output_2', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_3)\n",
        "    #nestnet_output_3 = layers.Conv2D(num_class, (1, 1), activation='sigmoid', name='output_3', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_4)\n",
        "    nestnet_output_4 = layers.Conv2D(num_class, (1, 1), activation='sigmoid', name='UNETP', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_5)\n",
        "\n",
        "    if deep_supervision:\n",
        "        model = Model(img_input, [nestnet_output_1,nestnet_output_2,nestnet_output_3,nestnet_output_4])\n",
        "    else:\n",
        "        model = Model(img_input, [nestnet_output_4], name=\"UNetp\")\n",
        "\n",
        "    # Pruning schedule\n",
        "    pruning_params = {\n",
        "        'pruning_schedule': sparsity.PolynomialDecay(\n",
        "            initial_sparsity=0.50,\n",
        "            final_sparsity=0.90,\n",
        "            begin_step=2000,\n",
        "            end_step=10000,\n",
        "            frequency=100\n",
        "        )\n",
        "    }\n",
        "\n",
        "    # Apply pruning to the model\n",
        "    pruned_model = sparsity.prune_low_magnitude(model, **pruning_params)\n",
        "\n",
        "    pruned_model.compile(optimizer='adam',\n",
        "                         loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                         metrics=['accuracy'])\n",
        "\n",
        "    return pruned_model\n",
        "callbacks = [\n",
        "    sparsity.UpdatePruningStep(),\n",
        "    sparsity.PruningSummaries(log_dir='./logs')\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWi7xmUiIqcs"
      },
      "source": [
        "DWT UNET++ (Standard & Residual, Our Net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unDHGAXGIlCL"
      },
      "outputs": [],
      "source": [
        "def DWTNestNet(img_rows, img_cols, color_type=1, num_class=1, deep_supervision=False):\n",
        "\n",
        "    nb_filter = [32,64,128,256,512,1024,2048]\n",
        "    bn_axis = 3\n",
        "    batch_norm= True\n",
        "    img_input = Input(shape=(img_rows, img_cols, color_type), name='main_input')\n",
        "\n",
        "    conv1_1 = standard_unit1(img_input, stage='11', nb_filter=nb_filter[0])\n",
        "    pool1 = DWT_Pooling()(conv1_1)\n",
        "\n",
        "    conv2_1 = standard_unit1(pool1, stage='21', nb_filter=nb_filter[1])\n",
        "    pool2 = DWT_Pooling()(conv2_1)\n",
        "\n",
        "    #a11=\tconv1_1, conv2_1, nb_filter[0] (x,g 32)\n",
        "    #gating_11 = gating_signal(conv2_1, nb_filter[0],batch_norm)\n",
        "    #a11 = attention_block(conv1_1, gating_11, nb_filter[0])\n",
        "    up1_2 = IWT_UpSampling()(conv2_1)\n",
        "    up1_2 = layers.Conv2D(filters = nb_filter[0], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up1_2)\n",
        "    print(up1_2.shape)\n",
        "    print(conv1_1.shape)\n",
        "    conv1_2 = Add()([up1_2, conv1_1])\n",
        "    conv1_2 = standard_unit1(conv1_2, stage='12', nb_filter=nb_filter[0])\n",
        "\n",
        "    conv3_1 = standard_unit1(pool2, stage='31', nb_filter=nb_filter[2])\n",
        "    pool3 = DWT_Pooling()(conv3_1)\n",
        "\n",
        "    up2_2 = IWT_UpSampling()(conv3_1)\n",
        "    up2_2 = layers.Conv2D(filters = nb_filter[1], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up2_2)\n",
        "    print(up2_2.shape)\n",
        "    print(conv2_1.shape)\n",
        "    conv2_2 = Add()([up2_2, conv2_1])\n",
        "    conv2_2 = standard_unit1(conv2_2, stage='22', nb_filter=nb_filter[1])\n",
        "\n",
        "    up1_3 = IWT_UpSampling()(conv2_2)\n",
        "    up1_3 = layers.Conv2D(filters = nb_filter[0], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up1_3)\n",
        "    print(up1_3.shape)\n",
        "    print(conv1_1.shape)\n",
        "    print(conv1_2.shape)\n",
        "    conv1_3 = Add()([up1_3, conv1_1, conv1_2])\n",
        "    conv1_3 = standard_unit1(conv1_3, stage='13', nb_filter=nb_filter[0])\n",
        "\n",
        "    conv4_1 = standard_unit1(pool3, stage='41', nb_filter=nb_filter[3])\n",
        "    pool4 = DWT_Pooling()(conv4_1)\n",
        "\n",
        "    up3_2 = IWT_UpSampling()(conv4_1)\n",
        "    up3_2 = layers.Conv2D(filters = nb_filter[2], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up3_2)\n",
        "    print(up3_2.shape)\n",
        "    print(conv3_1.shape)\n",
        "    conv3_2 = Add()([up3_2, conv3_1])\n",
        "    conv3_2 = standard_unit1(conv3_2, stage='32', nb_filter=nb_filter[2])\n",
        "\n",
        "    up2_3 = IWT_UpSampling()(conv3_2)\n",
        "    up2_3 = layers.Conv2D(filters = nb_filter[1], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up2_3)\n",
        "    print(up2_3.shape)\n",
        "    print(conv2_1.shape)\n",
        "    print(conv2_2.shape)\n",
        "    conv2_3 = Add()([up2_3, conv2_1, conv2_2])\n",
        "    conv2_3 = standard_unit1(conv2_3, stage='23', nb_filter=nb_filter[1])\n",
        "\n",
        "    up1_4 = IWT_UpSampling()(conv2_3)\n",
        "    up1_4 = layers.Conv2D(filters = nb_filter[0], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up1_4)\n",
        "    print(up1_4.shape)\n",
        "    print(conv1_1.shape)\n",
        "    print(conv1_2.shape)\n",
        "    print(conv1_3.shape)\n",
        "    conv1_4 = Add()([up1_4, conv1_1, conv1_2, conv1_3])\n",
        "    conv1_4 = standard_unit1(conv1_4, stage='14', nb_filter=nb_filter[0])\n",
        "\n",
        "    conv5_1 = standard_unit1(pool4, stage='51', nb_filter=nb_filter[4])\n",
        "\n",
        "    up4_2 = IWT_UpSampling()(conv5_1)\n",
        "    up4_2 = layers.Conv2D(filters = nb_filter[3], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up4_2)\n",
        "    print(up4_2.shape)#(None, 30, 40, 2048)\n",
        "\n",
        "    print(conv4_1.shape)#(None, 30, 40, 2048)\n",
        "    ####################################\n",
        "    conv4_2 = Add()([up4_2, conv4_1])\n",
        "    conv4_2 = standard_unit1(conv4_2, stage='42', nb_filter=nb_filter[3])\n",
        "\n",
        "    #a32= conv3_2, conv4_2, nb_filter[2] (x,g 128)\n",
        "    gating_32 = gating_signal(conv4_2, nb_filter[0], batch_norm)\n",
        "    a32 = attention_block(conv3_2, gating_32, nb_filter[2])\n",
        "    up3_3 = IWT_UpSampling()(conv4_2)\n",
        "    up3_3 = layers.Conv2D(filters = nb_filter[2], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up3_3)\n",
        "    print(up3_3.shape)\n",
        "    print(conv3_1.shape)\n",
        "    print(conv3_2.shape)\n",
        "    conv3_3 = Add()([up3_3, conv3_1, conv3_2])\n",
        "    conv3_3 = standard_unit1(conv3_3, stage='33', nb_filter=nb_filter[2])\n",
        "\n",
        "    up2_4 = IWT_UpSampling()(conv3_3)\n",
        "    up2_4 = layers.Conv2D(filters = nb_filter[1], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up2_4)\n",
        "    print(up2_4.shape)\n",
        "    print(conv2_1.shape)\n",
        "    print(conv2_2.shape)\n",
        "    print(conv2_3.shape)\n",
        "    conv2_4 = Add()([up2_4, conv2_1, conv2_2, conv2_3])\n",
        "    conv2_4 = standard_unit1(conv2_4, stage='24', nb_filter=nb_filter[1])\n",
        "\n",
        "    up1_5 = IWT_UpSampling()(conv2_4)\n",
        "    up1_5 = layers.Conv2D(filters = nb_filter[0], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up1_5)\n",
        "    print(up1_5.shape)\n",
        "    print(conv1_1.shape)\n",
        "    print(conv1_2.shape)\n",
        "    print(conv1_3.shape)\n",
        "    print(conv1_4.shape)\n",
        "    conv1_5 = Add()([up1_5, conv1_1, conv1_2, conv1_3, conv1_4])\n",
        "    conv1_5 = standard_unit1(conv1_5, stage='15', nb_filter=nb_filter[0])\n",
        "\n",
        "    #nestnet_output_1 = layers.Conv2D(num_class, (1, 1), activation='sigmoid', name='output_1', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_2)\n",
        "    #nestnet_output_2 = layers.Conv2D(num_class, (1, 1), activation='sigmoid', name='output_2', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_3)\n",
        "    #nestnet_output_3 = layers.Conv2D(num_class, (1, 1), activation='sigmoid', name='output_3', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_4)\n",
        "    #nestnet_output_4 = layers.Conv2D(num_class, (1, 1), activation='sigmoid', name='output_4', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_5)\n",
        "    nestnet_output_4 = layers.Conv2D(num_class, kernel_size=(1,1), kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_5)\n",
        "    nestnet_output_4 = layers.BatchNormalization(axis=3)(nestnet_output_4)\n",
        "    nestnet_output_4 = layers.Activation('sigmoid', name='NDWT')(nestnet_output_4)\n",
        "    #if deep_supervision:\n",
        "    #    model = Model(img_input, [nestnet_output_1,nestnet_output_2,nestnet_output_3,nestnet_output_4])\n",
        "    #else:\n",
        "    model = Model(img_input, [nestnet_output_4],name=\"DWT-UNetP\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_sP1UDtcHD6"
      },
      "source": [
        "(None, 240, 320, 256)\n",
        "(None, 240, 320, 256)\n",
        "\n",
        "(None, 120, 160, 512)\n",
        "(None, 120, 160, 512)\n",
        "\n",
        "(None, 240, 320, 256)\n",
        "(None, 240, 320, 256)\n",
        "(None, 240, 320, 256)\n",
        "\n",
        "(None, 60, 80, 1024)\n",
        "(None, 60, 80, 1024)\n",
        "\n",
        "(None, 120, 160, 512)\n",
        "(None, 120, 160, 512)\n",
        "(None, 120, 160, 512)\n",
        "\n",
        "(None, 240, 320, 256)\n",
        "(None, 240, 320, 256)\n",
        "(None, 240, 320, 256)\n",
        "(None, 240, 320, 256)\n",
        "\n",
        "(None, 30, 40, 2048)\n",
        "(None, 30, 40, 2048)\n",
        "\n",
        "(None, 60, 80, 1024)\n",
        "(None, 60, 80, 1024)\n",
        "(None, 60, 80, 1024)\n",
        "\n",
        "(None, 120, 160, 512)\n",
        "(None, 120, 160, 512)\n",
        "(None, 120, 160, 512)\n",
        "(None, 120, 160, 512)\n",
        "\n",
        "(None, 240, 320, 256)\n",
        "(None, 240, 320, 256)\n",
        "(None, 240, 320, 256)\n",
        "(None, 240, 320, 256)\n",
        "(None, 240, 320, 256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxsa3Y2EbvSD"
      },
      "source": [
        "# EXPIREMENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VifKB7bHbtmD"
      },
      "outputs": [],
      "source": [
        "def DWTNestNet(img_rows, img_cols, color_type=1, num_class=1, deep_supervision=False):\n",
        "    #added 32 to nb filter\n",
        "    nb_filter = [32,64,128,256,512,1024,2048]\n",
        "    bn_axis = 3\n",
        "    batch_norm= True\n",
        "    img_input = Input(shape=(img_rows, img_cols, color_type), name='main_input')\n",
        "\n",
        "    conv1_1 = standard_unit1(img_input, stage='11', nb_filter=nb_filter[0])\n",
        "    pool1 = DWT_Pooling()(conv1_1)\n",
        "\n",
        "    conv2_1 = standard_unit1(pool1, stage='21', nb_filter=nb_filter[1])\n",
        "    pool2 = DWT_Pooling()(conv2_1)\n",
        "\n",
        "    #a11=\tconv1_1, conv2_1, nb_filter[0] (x,g 32)\n",
        "    #gating_11 = gating_signal(conv2_1, nb_filter[0],batch_norm)\n",
        "    #a11 = attention_block(conv1_1, gating_11, nb_filter[0])\n",
        "    up1_2 = IWT_UpSampling()(conv2_1)\n",
        "    up1_2 = layers.Conv2D(filters = nb_filter[0], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up1_2)\n",
        "    print(up1_2.shape)\n",
        "    print(conv1_1.shape)\n",
        "    conv1_2 = Add()([up1_2, conv1_1])\n",
        "    conv1_2 = standard_unit1(conv1_2, stage='12', nb_filter=nb_filter[0])\n",
        "\n",
        "    conv3_1 = standard_unit1(pool2, stage='31', nb_filter=nb_filter[2])\n",
        "    pool3 = DWT_Pooling()(conv3_1)\n",
        "\n",
        "    up2_2 = IWT_UpSampling()(conv3_1)\n",
        "    up2_2 = layers.Conv2D(filters = nb_filter[3], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up2_2)\n",
        "    print(up2_2.shape)\n",
        "    print(conv2_1.shape)\n",
        "    conv2_2 = Add()([up2_2, conv2_1])\n",
        "    conv2_2 = standard_unit1(conv2_2, stage='22', nb_filter=nb_filter[1])\n",
        "\n",
        "    up1_3 = IWT_UpSampling()(conv2_2)\n",
        "    up1_3 = layers.Conv2D(filters = nb_filter[2], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up1_3)\n",
        "    print(up1_3.shape)\n",
        "    print(conv1_1.shape)\n",
        "    print(conv1_2.shape)\n",
        "    conv1_3 = Add()([up1_3, conv1_1, conv1_2])\n",
        "    conv1_3 = standard_unit1(conv1_3, stage='13', nb_filter=nb_filter[0])\n",
        "\n",
        "    conv4_1 = standard_unit1(pool3, stage='41', nb_filter=nb_filter[2])\n",
        "    pool4 = DWT_Pooling()(conv4_1)\n",
        "\n",
        "    up3_2 = IWT_UpSampling()(conv4_1)\n",
        "    up3_2 = layers.Conv2D(filters = nb_filter[4], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up3_2)\n",
        "    print(up3_2.shape)\n",
        "    print(conv3_1.shape)\n",
        "    conv3_2 = Add()([up3_2, conv3_1])\n",
        "    conv3_2 = standard_unit1(conv3_2, stage='32', nb_filter=nb_filter[2])\n",
        "\n",
        "    up2_3 = IWT_UpSampling()(conv3_2)\n",
        "    up2_3 = layers.Conv2D(filters = nb_filter[3], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up2_3)\n",
        "    print(up2_3.shape)\n",
        "    print(conv2_1.shape)\n",
        "    print(conv2_2.shape)\n",
        "    conv2_3 = Add()([up2_3, conv2_1, conv2_2])\n",
        "    conv2_3 = standard_unit1(conv2_3, stage='23', nb_filter=nb_filter[1])\n",
        "\n",
        "    up1_4 = IWT_UpSampling()(conv2_3)\n",
        "    up1_4 = layers.Conv2D(filters = nb_filter[2], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up1_4)\n",
        "    print(up1_4.shape)\n",
        "    print(conv1_1.shape)\n",
        "    print(conv1_2.shape)\n",
        "    print(conv1_3.shape)\n",
        "    conv1_4 = Add()([up1_4, conv1_1, conv1_2, conv1_3])\n",
        "    conv1_4 = standard_unit1(conv1_4, stage='14', nb_filter=nb_filter[0])\n",
        "\n",
        "    conv5_1 = standard_unit1(pool4, stage='51', nb_filter=nb_filter[4])\n",
        "\n",
        "    up4_2 = IWT_UpSampling()(conv5_1)\n",
        "    up4_2 = layers.Conv2D(filters = nb_filter[4], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up4_2)\n",
        "    print(up4_2.shape)#(None, 30, 40, 2048)\n",
        "\n",
        "    print(conv4_1.shape)#(None, 30, 40, 2048)\n",
        "    ####################################\n",
        "    conv4_2 = Add()([up4_2, conv4_1])\n",
        "    conv4_2 = standard_unit1(conv4_2, stage='42', nb_filter=nb_filter[3])\n",
        "\n",
        "    #a32= conv3_2, conv4_2, nb_filter[2] (x,g 128)\n",
        "    gating_32 = gating_signal(conv4_2, nb_filter[2], batch_norm)\n",
        "    a32 = attention_block(conv3_2, gating_32, nb_filter[2])\n",
        "    up3_3 = IWT_UpSampling()(conv4_2)\n",
        "    up3_3 = layers.Conv2D(filters = nb_filter[4], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up3_3)\n",
        "    print(up3_3.shape)\n",
        "    print(conv3_1.shape)\n",
        "    print(conv3_2.shape)\n",
        "    conv3_3 = Add()([up3_3, conv3_1, conv3_2])\n",
        "    conv3_3 = standard_unit1(conv3_3, stage='33', nb_filter=nb_filter[2])\n",
        "\n",
        "    up2_4 = IWT_UpSampling()(conv3_3)\n",
        "    up2_4 = layers.Conv2D(filters = nb_filter[3], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up2_4)\n",
        "    print(up2_4.shape)\n",
        "    print(conv2_1.shape)\n",
        "    print(conv2_2.shape)\n",
        "    print(conv2_3.shape)\n",
        "    conv2_4 = Add()([up2_4, conv2_1, conv2_2, conv2_3])\n",
        "    conv2_4 = standard_unit1(conv2_4, stage='24', nb_filter=nb_filter[1])\n",
        "\n",
        "    up1_5 = IWT_UpSampling()(conv2_4)\n",
        "    up1_5 = layers.Conv2D(filters = nb_filter[2], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up1_5)\n",
        "    print(up1_5.shape)\n",
        "    print(conv1_1.shape)\n",
        "    print(conv1_2.shape)\n",
        "    print(conv1_3.shape)\n",
        "    print(conv1_4.shape)\n",
        "    conv1_5 = Add()([up1_5, conv1_1, conv1_2, conv1_3, conv1_4])\n",
        "    conv1_5 = standard_unit1(conv1_5, stage='15', nb_filter=nb_filter[0])\n",
        "\n",
        "    #nestnet_output_1 = layers.Conv2D(num_class, (1, 1), activation='sigmoid', name='output_1', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_2)\n",
        "    #nestnet_output_2 = layers.Conv2D(num_class, (1, 1), activation='sigmoid', name='output_2', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_3)\n",
        "    #nestnet_output_3 = layers.Conv2D(num_class, (1, 1), activation='sigmoid', name='output_3', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_4)\n",
        "    #nestnet_output_4 = layers.Conv2D(num_class, (1, 1), activation='sigmoid', name='output_4', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_5)\n",
        "    nestnet_output_4 = layers.Conv2D(num_class, kernel_size=(1,1), kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_5)\n",
        "    nestnet_output_4 = layers.BatchNormalization(axis=3)(nestnet_output_4)\n",
        "    nestnet_output_4 = layers.Activation('sigmoid', name='NDWT')(nestnet_output_4)\n",
        "    #if deep_supervision:\n",
        "    #    model = Model(img_input, [nestnet_output_1,nestnet_output_2,nestnet_output_3,nestnet_output_4])\n",
        "    #else:\n",
        "    model = Model(img_input, [nestnet_output_4],name=\"DWT-UNetP\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AT4DaLw4GnmF"
      },
      "outputs": [],
      "source": [
        "model =  DWTNestNet(HEIGHT, WIDTH, color_type=3, num_class=1, deep_supervision=False)\n",
        "#model  = ADWTNestNet(HEIGHT, WIDTH, color_type=3, num_class=1, deep_supervision=False)\n",
        "#model  = RNest_Net(HEIGHT, WIDTH, color_type=3, num_class=1, deep_supervision=False)\n",
        "model.compile(optimizer=opt, loss=depth_loss, metrics=[depth_acc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_D_wGD8embCS"
      },
      "source": [
        "DWT Atten UNET++ (Standard & Residual, Our Net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMTWMk33PhHb"
      },
      "outputs": [],
      "source": [
        "def ADWTNestNet(img_rows, img_cols, color_type=1, num_class=1, deep_supervision=False):\n",
        "    #added 32 to nb filter\n",
        "    nb_filter = [32,64,128,256,512,1024,2048]\n",
        "    bn_axis = 3\n",
        "    batch_norm= True\n",
        "    img_input = Input(shape=(img_rows, img_cols, color_type), name='main_input')\n",
        "\n",
        "    conv1_1 = standard_unit1(img_input, stage='11', nb_filter=nb_filter[0])\n",
        "    pool1 = DWT_Pooling()(conv1_1)\n",
        "\n",
        "    conv2_1 = standard_unit1(pool1, stage='21', nb_filter=nb_filter[1])\n",
        "    pool2 = DWT_Pooling()(conv2_1)\n",
        "\n",
        "    #a11=\tconv1_1, conv2_1, nb_filter[0] (x,g 32)\n",
        "    gating_11 = gating_signal(conv2_1, nb_filter[0],batch_norm)\n",
        "    a11 = attention_block(conv1_1, gating_11, nb_filter[2])#0\n",
        "    print(a11.shape)\n",
        "    up1_2 = IWT_UpSampling()(conv2_1)\n",
        "    up1_2 = layers.Conv2D(filters = nb_filter[0], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up1_2)\n",
        "    print(up1_2.shape)\n",
        "    conv1_2 = Add()([up1_2, a11])\n",
        "    conv1_2 = standard_unit1(conv1_2, stage='12', nb_filter=nb_filter[0])\n",
        "\n",
        "    conv3_1 = standard_unit1(pool2, stage='31', nb_filter=nb_filter[2])\n",
        "    pool3 = DWT_Pooling()(conv3_1)\n",
        "\n",
        "    #a21= conv2_1, conv3_1, nb_filter[0] (x,g 64)\n",
        "    gating_21 = gating_signal(conv3_1, nb_filter[1], batch_norm)\n",
        "    a21 = attention_block(conv2_1, gating_21, nb_filter[2])#2\n",
        "    up2_2 = IWT_UpSampling()(conv3_1)\n",
        "    up2_2 = layers.Conv2D(filters = nb_filter[1], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up2_2)\n",
        "    print(a21.shape)\n",
        "    print(up2_2.shape)\n",
        "    conv2_2 = Add()([up2_2, a21])\n",
        "    conv2_2 = standard_unit1(conv2_2, stage='22', nb_filter=nb_filter[1])\n",
        "\n",
        "    #a12= conv1_2, conv2_2, nb_filter[0] (x,g 32)\n",
        "    gating_12 = gating_signal(conv2_2, nb_filter[0], batch_norm)\n",
        "    a12 = attention_block(conv1_2, gating_12, nb_filter[0])\n",
        "    up1_3 = IWT_UpSampling()(conv2_2)\n",
        "    up1_3 = layers.Conv2D(filters = nb_filter[0], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up1_3)\n",
        "    print(a11.shape)\n",
        "    print(a12.shape)\n",
        "    print(up1_3.shape)\n",
        "\n",
        "    conv1_3 = Add()([up1_3, a11, a12])\n",
        "    conv1_3 = standard_unit1(conv1_3, stage='13', nb_filter=nb_filter[0])\n",
        "\n",
        "    conv4_1 = standard_unit1(pool3, stage='41', nb_filter=nb_filter[3])\n",
        "    pool4 = DWT_Pooling()(conv4_1)\n",
        "\n",
        "    #a31= conv3_1, conv4_1, nb_filter[2] (x,g 128)\n",
        "    gating_31 = gating_signal(conv4_1, nb_filter[2], batch_norm)\n",
        "    a31 = attention_block(conv3_1, gating_31, nb_filter[2])\n",
        "    up3_2 = IWT_UpSampling()(conv4_1)\n",
        "    up3_2 = layers.Conv2D(filters = nb_filter[2], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up3_2)\n",
        "    print(a31.shape)\n",
        "    print(up3_2.shape)\n",
        "    conv3_2 = Add()([up3_2, a31])\n",
        "    conv3_2 = standard_unit1(conv3_2, stage='32', nb_filter=nb_filter[2])\n",
        "\n",
        "\n",
        "    #a22= conv2_2, conv3_2, nb_filter[1] (x,g 64)\n",
        "    gating_22 = gating_signal(conv3_2, nb_filter[1], batch_norm)\n",
        "    a22 = attention_block(conv2_2, gating_22, nb_filter[1])\n",
        "    up2_3 = IWT_UpSampling()(conv3_2)\n",
        "    up2_3 = layers.Conv2D(filters = nb_filter[1], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up2_3)\n",
        "    print(a21.shape)\n",
        "    print(a22.shape)\n",
        "    print(up2_3.shape)\n",
        "    conv2_3 = Add()([up2_3, a21, a22])\n",
        "    conv2_3 = standard_unit1(conv2_3, stage='23', nb_filter=nb_filter[1])\n",
        "\n",
        "    #a13= conv1_3, conv2_3, nb_filter[0] (x,g 32)\n",
        "    gating_13 = gating_signal(conv2_3, nb_filter[0], batch_norm)\n",
        "    a13 = attention_block(conv1_3, gating_13, nb_filter[0])\n",
        "    up1_4 = IWT_UpSampling()(conv2_3)\n",
        "    up1_4 = layers.Conv2D(filters = nb_filter[0], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up1_4)\n",
        "    print(up1_4.shape)\n",
        "    print(a11.shape)\n",
        "    print(a12.shape)\n",
        "    print(a13.shape)\n",
        "    conv1_4 = Add()([up1_4, a11,a12, a13])\n",
        "    conv1_4 = standard_unit1(conv1_4, stage='14', nb_filter=nb_filter[0])\n",
        "\n",
        "    conv5_1 = standard_unit1(pool4, stage='51', nb_filter=nb_filter[4])\n",
        "\n",
        "    #a41= conv4_1, conv5_1, nb_filter[2] (x,g 256)\n",
        "    gating_41 = gating_signal(conv5_1, nb_filter[2], batch_norm)\n",
        "    a41 = attention_block(conv4_1, gating_41, nb_filter[2])\n",
        "    up4_2 = IWT_UpSampling()(conv5_1)\n",
        "    up4_2 = layers.Conv2D(filters = nb_filter[3], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up4_2)\n",
        "    print(up4_2.shape)\n",
        "    print(a41.shape)\n",
        "    conv4_2 = Add()([up4_2, a41])\n",
        "    conv4_2 = standard_unit1(conv4_2, stage='42', nb_filter=nb_filter[3])\n",
        "    #\n",
        "    #\n",
        "    #\n",
        "    #\n",
        "\n",
        "\n",
        "    #a32= conv3_2, conv4_2, nb_filter[2] (x,g 128)\n",
        "    gating_32 = gating_signal(conv4_2, nb_filter[2], batch_norm)\n",
        "    a32 = attention_block(conv3_2, gating_32, nb_filter[2])\n",
        "    up3_3 = IWT_UpSampling()(conv4_2)\n",
        "    up3_3 = layers.Conv2D(filters = nb_filter[2], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up3_3)\n",
        "    print(up3_3.shape)\n",
        "    print(a31.shape)\n",
        "    print(a32.shape)\n",
        "    conv3_3 = Add()([up3_3, a31, a32])\n",
        "    conv3_3 = standard_unit1(conv3_3, stage='33', nb_filter=nb_filter[2])\n",
        "\n",
        "    #a23= conv2_3, conv3_3, nb_filter[1] (x,g 64)\n",
        "    gating_23 = gating_signal(conv3_3, nb_filter[1], batch_norm)\n",
        "    a23 = attention_block(conv2_3, gating_23, nb_filter[1])\n",
        "    up2_4 = IWT_UpSampling()(conv3_3)\n",
        "    up2_4 = layers.Conv2D(filters = nb_filter[1], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up2_4)\n",
        "    print(up2_4.shape)\n",
        "    print(a21.shape)\n",
        "    print(a22.shape)\n",
        "    print(a23.shape)\n",
        "    conv2_4 = Add()([up2_4, a21, a22, a23])\n",
        "    conv2_4 = standard_unit1(conv2_4, stage='24', nb_filter=nb_filter[1])\n",
        "\n",
        "    #a14= conv1_4, conv2_4, nb_filter[0] (x,g 32)\n",
        "    gating_14 = gating_signal(conv2_4, nb_filter[0], batch_norm)\n",
        "    a14 = attention_block(conv1_4, gating_14, nb_filter[0])\n",
        "    up1_5 = IWT_UpSampling()(conv2_4)\n",
        "    up1_5 = layers.Conv2D(filters = nb_filter[0], kernel_size = (1, 1),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(up1_5)\n",
        "    print(up1_5.shape)\n",
        "    print(a11.shape)\n",
        "    print(a12.shape)\n",
        "    print(a13.shape)\n",
        "    print(a14.shape)\n",
        "    conv1_5 = Add()([up1_5, a11, a12, a13, a14])\n",
        "    conv1_5 = standard_unit1(conv1_5, stage='15', nb_filter=nb_filter[0])\n",
        "\n",
        "    #nestnet_output_1 = layers.Conv2D(num_class, (1, 1), activation='sigmoid', name='output_1', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_2)\n",
        "    #nestnet_output_2 = layers.Conv2D(num_class, (1, 1), activation='sigmoid', name='output_2', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_3)\n",
        "    #nestnet_output_3 = layers.Conv2D(num_class, (1, 1), activation='sigmoid', name='output_3', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_4)\n",
        "    #nestnet_output_4 = layers.Conv2D(num_class, (1, 1), activation='sigmoid', name='output_4', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_5)\n",
        "    nestnet_output_4 = layers.Conv2D(num_class, kernel_size=(1,1), kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_5)\n",
        "    nestnet_output_4 = layers.BatchNormalization(axis=3)(nestnet_output_4)\n",
        "    nestnet_output_4 = layers.Activation('sigmoid', name='NDWT')(nestnet_output_4)\n",
        "    #if deep_supervision:\n",
        "    #    model = Model(img_input, [nestnet_output_1,nestnet_output_2,nestnet_output_3,nestnet_output_4])\n",
        "    #else:\n",
        "    model = Model(img_input, [nestnet_output_4],name=\"AttnDWT-UNetP\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkSZfsaI8_A6"
      },
      "source": [
        "(None, 240, 320, 256)\n",
        "(None, 240, 320, 256)\n",
        "\n",
        "(None, 120, 160, 512)\n",
        "(None, 120, 160, 512)\n",
        "\n",
        "(None, 240, 320, 256)\n",
        "(None, 240, 320, 256)\n",
        "(None, 240, 320, 256)\n",
        "\n",
        "(None, 60, 80, 1024)\n",
        "(None, 60, 80, 1024)\n",
        "\n",
        "(None, 120, 160, 512)\n",
        "(None, 120, 160, 512)\n",
        "(None, 120, 160, 512)\n",
        "\n",
        "(None, 240, 320, 256)\n",
        "(None, 240, 320, 256)\n",
        "(None, 240, 320, 256)\n",
        "(None, 240, 320, 256)\n",
        "\n",
        "(None, 30, 40, 2048)\n",
        "(None, 30, 40, 2048)\n",
        "\n",
        "(None, 60, 80, 1024)\n",
        "(None, 60, 80, 1024)\n",
        "(None, 60, 80, 1024)\n",
        "\n",
        "(None, 120, 160, 512)\n",
        "(None, 120, 160, 512)\n",
        "(None, 120, 160, 512)\n",
        "(None, 120, 160, 512)\n",
        "\n",
        "(None, 240, 320, 256)\n",
        "(None, 240, 320, 256)\n",
        "(None, 240, 320, 256)\n",
        "(None, 240, 320, 256)\n",
        "(None, 240, 320, 256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntXzetXqENY3"
      },
      "source": [
        "## Defining the loss\n",
        "\n",
        "We will optimize 3 losses in our mode.\n",
        "1. Structural similarity index(SSIM).\n",
        "2. L1-loss, or Point-wise depth in our case.\n",
        "3. Depth smoothness loss.\n",
        "\n",
        "Out of the three loss functions, SSIM contributes the most to improving model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaJtndx03Zu1"
      },
      "outputs": [],
      "source": [
        "@keras.saving.register_keras_serializable(package=\"poly_decay\")\n",
        "def poly_decay(epoch):\n",
        "  maxEpochs = EPOCHS\n",
        "  baseLR = INIT_LR\n",
        "  power = 1.0\n",
        "  alpha = baseLR * (1 - (epoch / float(maxEpochs))) ** power\n",
        "  return alpha\n",
        "\n",
        "opt = Adam(learning_rate=INIT_LR, amsgrad=True)\n",
        "\n",
        "checkpoint_path = \"//content/drive/MyDrive/NDWTw3/cp.ckpt\"\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                save_weights_only=True,\n",
        "                                                save_best_only=True,\n",
        "                                                verbose=1)\n",
        "callbacks = [LearningRateScheduler(poly_decay),checkpoint]\n",
        "\n",
        "# custom loss\n",
        "\n",
        "@keras.saving.register_keras_serializable(package=\"depth_loss\")\n",
        "def depth_loss(y_true, y_pred):\n",
        "  w1, w2, w3 = 1.0, 1.0, 0.2\n",
        "\n",
        "  l_depth = K.mean(K.abs(y_pred - y_true), axis=-1)\n",
        "  '''abs_error = tf.abs(tf.subtract(y_pred,y_true)) # log\n",
        "  c = 0.2 * tf.reduce_max(abs_error)\n",
        "  berHu_loss1 = tf.where(abs_error <= c,\n",
        "                  abs_error,\n",
        "                  (tf.square(abs_error) + tf.square(c))/(2*c))\n",
        "  l_depth = K.mean(berHu_loss1)'''\n",
        "\n",
        "  #dy_true, dx_true = tf.image.image_gradients(y_true)\n",
        "  #dy_pred, dx_pred = tf.image.image_gradients(y_pred)\n",
        "  #l_edges = K.mean(K.abs(dy_pred - dy_true) + K.abs(dx_pred - dx_true), axis=-1)\n",
        "\n",
        "  l_ssim = K.clip((1 - tf.image.ssim(y_true, y_pred, 1.0)) * 0.5, 0, 1)\n",
        "\n",
        "  return (w1 * l_ssim) + (w3 * K.mean(l_depth))#(w2 * K.mean(l_edges)) +\n",
        "\n",
        "#custom soft accuracy\n",
        "@keras.saving.register_keras_serializable(package=\"depth_acc\")\n",
        "def depth_acc(y_true, y_pred):\n",
        "  return K.mean(K.equal(K.round(y_true), K.round(y_pred)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0KuUF78M-4z"
      },
      "source": [
        "Another version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GE3YUfgwxJq"
      },
      "outputs": [],
      "source": [
        "ssim_loss_weight = 0.85\n",
        "l1_loss_weight = 0.1\n",
        "edge_loss_weight = 0.9\n",
        "\n",
        "def train_loss(y_true, y_pred):\n",
        "  theta=0.1\n",
        "  maxDepthVal=1000.0/10.0\n",
        "  LapFilter = K.constant([[[[0]], [[-1 ]], [[0 ]]], [[[-1]], [[5 ]], [[-1 ]]], [[[0 ]], [[-1 ]], [[0 ]]]])\n",
        "  #Cosine distance loss\n",
        "  #l_depth = K.mean(K.abs(y_pred - y_true), axis=-1)\n",
        "  abs_error = tf.abs(tf.subtract(y_pred,y_true)) # log\n",
        "  c = 0.2 * tf.reduce_max(abs_error)\n",
        "  berHu_loss1 = tf.where(abs_error <= c,\n",
        "                  abs_error,\n",
        "                  (tf.square(abs_error) + tf.square(c))/(2*c))\n",
        "  l_depth = K.mean(berHu_loss1)\n",
        "  # edge loss for sharp edges\n",
        "  #dy_true, dx_true = tf.image.image_gradients(y_true)\n",
        "  #dy_pred, dx_pred = tf.image.image_gradients(y_pred)\n",
        "  #l_edges = K.mean(K.abs(dy_pred - dy_true) + K.abs(dx_pred - dx_true), axis=-1)\n",
        "  s_true  = tf.image.sobel_edges(y_true)\n",
        "  s_pred  = tf.image.sobel_edges(y_pred)\n",
        "  l_edges = K.mean(K.mean(K.abs((s_pred[...,0]/4+0.5) - (s_true[...,0]/4+0.5)) + K.abs((s_pred[...,1]/4+0.5) - (s_true[...,1]/4+0.5)), axis=-1))\n",
        "  # structural similarity loss\n",
        "  inputChannels = K.reshape(K.ones_like(y_true[0,0,0,:]),(1,1,-1,1))\n",
        "  filt = LapFilter * inputChannels\n",
        "  LapTrue = K.depthwise_conv2d(y_true,filt, padding='same' )\n",
        "  LapTrue =tf.clip_by_value(LapTrue, clip_value_min=0.0, clip_value_max=1.0)\n",
        "  l_ssim = K.clip((1 - tf.image.ssim(LapTrue, y_pred, maxDepthVal)) * 0.5, 0, 1)\n",
        "  #l_ssim = K.clip((1 - tf.image.ssim(y_true, y_pred, 1.0)) * 1, 0, 1)\n",
        "\n",
        "  # weightage\n",
        "  w1, w2, w3 = 1.0, 1.0, 0.1\n",
        "  return (w1 * l_ssim) + (w2 * K.mean(l_edges)) + (w3 * K.mean(l_depth))\n",
        "\n",
        "#optimizer\n",
        "#opt = tf.keras.optimizers.Adam(learning_rate=0.0001, decay=1e-6,amsgrad=True)\n",
        "opt = tfa.optimizers.AdamW(learning_rate=0.0001, weight_decay=1e-6,amsgrad=True)\n",
        "# accuracy function\n",
        "def accuracy_function(y_true, y_pred):\n",
        "  return K.mean(K.equal(K.round(y_true), K.round(y_pred)))\n",
        "\n",
        "# save model frequently for later use.\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                save_weights_only=True,\n",
        "                                                save_best_only=True,\n",
        "                                                verbose=1)\n",
        "# Learning rate scheduler\n",
        "def polynomial_decay(epoch):\n",
        "  max_epochs = EPOCHS\n",
        "  #base_lr =  INIT_LR\n",
        "  power = 1.0\n",
        "  lr = INIT_LR * (1 - (epoch / float(max_epochs))) ** power\n",
        "  return lr\n",
        "\n",
        "callbacks = [LearningRateScheduler(polynomial_decay, verbose=1), checkpoint]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVXqNXQIENZC"
      },
      "source": [
        "## Possible improvements\n",
        "\n",
        "1. You can improve this model by replacing the encoding part of the U-Net with a\n",
        "pretrained DenseNet or ResNet.\n",
        "2. Loss functions play an important role in solving this problem.\n",
        "Tuning the loss functions may yield significant improvement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6upN-OPeENY7"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bVfXzej4iLy"
      },
      "outputs": [],
      "source": [
        "# start = time.time()\n",
        "#model = UnetModel()\n",
        "#model = UNet(HEIGHT, WIDTH, color_type=3, num_class=1)\n",
        "#model = AUNet(256, 256, color_type=3, num_class=1)\n",
        "#model = unetWavelet()\n",
        "#model = unetWavelet(HEIGHT, WIDTH, 3)\n",
        "#model = Udwt(HEIGHT, WIDTH, color_type=3, num_class=1)\n",
        "#model = AUdwt(256, 256, color_type=3, num_class=1)\n",
        "#model = Nest_Net(HEIGHT, WIDTH, color_type=3, num_class=1)\n",
        "#model = RNest_Net(256, 256, color_type=3, num_class=1)\n",
        "#model = ARNest_Net(256, 256, color_type=3, num_class=1)\n",
        "#odel =  DWTNestNet(HEIGHT, WIDTH, color_type=3, num_class=1, deep_supervision=False)\n",
        "model  = ADWTNestNet(HEIGHT, WIDTH, color_type=3, num_class=1, deep_supervision=False)\n",
        "#model  = RNest_Net(HEIGHT, WIDTH, color_type=3, num_class=1, deep_supervision=False)\n",
        "model.compile(optimizer=opt, loss=depth_loss, metrics=[depth_acc])\n",
        "# model.summary()\n",
        "# end = time.time()\n",
        "# print('\\nTest time', end-start, 's')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "asoHP3EtBQB2"
      },
      "outputs": [],
      "source": [
        "r = model.fit(training_generator, validation_data=validation_generator, epochs=EPOCHS, callbacks=callbacks)\n",
        "#!mkdir -p saved_model\n",
        "model.save('/content/drive/MyDrive/colab/saved_models/keras/dwtnestnetstand14.keras')\n",
        "model.save('/content/drive/MyDrive/colab/saved_models/h5/dwtnestnetstand14.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO9y-5k14i9y"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "48A7OOvH3TFu"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wok7KjyD4rnD"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "# e = np.linspace(1, EPOCHS, num=EPOCHS)\n",
        "\n",
        "plt.plot(r.history['loss'], label='loss')\n",
        "plt.plot(r.history['val_loss'], label='val_loss')\n",
        "\n",
        "plt.plot(r.history['depth_acc'], label='acc')\n",
        "plt.plot(r.history['val_depth_acc'], label='val_acc')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfzxIH_bTaTZ"
      },
      "source": [
        "FLOPs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87-Oi2btDWoy"
      },
      "source": [
        "#FLOPs & MAC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDQ3ux-I9CIN"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.profiler.model_analyzer import profile\n",
        "from tensorflow.python.profiler.option_builder import ProfileOptionBuilder\n",
        "\n",
        "def get_flops(model):\n",
        "  forward_pass = tf.function(model.call, input_signature=[tf.TensorSpec(shape=(1,) + model.input_shape[1:])])\n",
        "  graph_info = profile(forward_pass.get_concrete_function().graph, options=ProfileOptionBuilder.float_operation())\n",
        "  flops = graph_info.total_float_ops\n",
        "  return flops\n",
        "\n",
        "#model = tf.keras.applications.ConvNeXtTiny()\n",
        "\n",
        "# model.compile(optimizer='adam', loss='bce', metrics=['accuracy'])\n",
        "\n",
        "#flops = get_flops(new_model)\n",
        "flops = get_flops(model)\n",
        "macs = flops / 2\n",
        "print(f\"MACs: {macs / 1e+9:,} G\")\n",
        "print(f\"FLOPs: {flops / 1e+9:,} G\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyiPeqbsNELu"
      },
      "source": [
        "[link text](https://)## Evaluation for NYU Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vA-pHpI2MPZB"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.load_model('/content/drive/MyDrive/colab/saved_models/keras/NestNet4ince.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaJkLLK15PRF"
      },
      "outputs": [],
      "source": [
        "img_dm_pairs = read_csv('/content/data/nyu2_test.csv')\n",
        "labels = {i: j for i,j in img_dm_pairs}\n",
        "test_paths = [i for i,j in img_dm_pairs]\n",
        "print(len(test_paths))\n",
        "partition = {'test': test_paths}\n",
        "\n",
        "x_test = np.empty((len(test_paths), HEIGHT, WIDTH, 3))\n",
        "y_test = np.empty((len(test_paths), HEIGHT, WIDTH, 1))\n",
        "\n",
        "for i, ID in enumerate(partition['test'][:]):\n",
        "  x_test[i, ] = preprocess_image(ID, horizontal_flip=False)\n",
        "  y_test[i, ] = preprocess_depth_map(labels[ID], horizontal_flip=False)\n",
        "print(model.evaluate(x_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGmyzFzTGP71"
      },
      "outputs": [],
      "source": [
        "from matplotlib.patches import BoxStyle\n",
        "def compute_errors(gt, pred):\n",
        "    eps = 1e-9 #to keep values below 1\n",
        "    thresh = np.maximum((gt / (pred+eps)), (pred / (gt+eps)))\n",
        "    a1 = (thresh < 1.25   ).mean()\n",
        "    a2 = (thresh < 1.25 ** 2).mean()\n",
        "    a3 = (thresh < 1.25 ** 3).mean()\n",
        "    abs_rel = np.mean(np.abs(gt - pred) / (gt+eps))\n",
        "    rmse = (gt - pred) ** 2\n",
        "    rmse = np.sqrt(rmse.mean())\n",
        "    log_10 = (np.abs(np.log10(gt+eps)-np.log10(pred+eps))).mean()\n",
        "    return a1, a2, a3, abs_rel, rmse, log_10\n",
        "\n",
        "def DepthNorm(x, maxDepth):\n",
        "    return maxDepth / x\n",
        "\n",
        "def scale_up(scale, images):\n",
        "    from skimage.transform import resize\n",
        "    scaled = []\n",
        "\n",
        "    for i in range(len(images)):\n",
        "        img = images[i]\n",
        "        output_shape = (scale * img.shape[0], scale * img.shape[1])\n",
        "        scaled.append( resize(img, output_shape, order=1, preserve_range=True, mode='reflect', anti_aliasing=True ) )\n",
        "        print(output_shape.shape)\n",
        "    return np.stack(scaled)\n",
        "\n",
        "def predict(model, images, minDepth=10, maxDepth=1000, batch_size=2):\n",
        "    # Support multiple RGBs, one RGB image, even grayscale\n",
        "    if len(images.shape) < 3: images = np.stack((images,images,images), axis=2)\n",
        "    if len(images.shape) < 4: images = images.reshape((1, images.shape[0], images.shape[1], images.shape[2]))\n",
        "    # Compute predictions\n",
        "    predictions = model.predict(images, batch_size=batch_size)\n",
        "    # Put in expected range\n",
        "    return np.clip(DepthNorm(predictions, maxDepth=1000), minDepth, maxDepth) / maxDepth\n",
        "\n",
        "def evaluate(model, rgb, depth, crop, batch_size=6, verbose=False):\n",
        "    N = len(rgb)\n",
        "    bs = batch_size\n",
        "    predictions = []\n",
        "    testSetDepths = []\n",
        "\n",
        "\n",
        " #Compute results\n",
        "    for i in range(N//bs):\n",
        "        x = rgb[(i)*bs:(i+1)*bs,:,:,:]\n",
        "        x = tf.image.resize(x, [240, 320])\n",
        "        # Compute results\n",
        "        true_y = depth[(i)*bs:(i+1)*bs,:,:]\n",
        "        true_y = np.expand_dims(true_y, -1)\n",
        "        true_y = tf.image.resize(true_y, [240,320])\n",
        "        ##pred_y = scale_up(1, predict(model, x/255, minDepth=10, maxDepth=1000, batch_size=bs)[:,:,:,0]) * 10.0\n",
        "        pred_y = (model.predict(x/255, batch_size=bs)) #*10\n",
        "        pred_y = np.clip(DepthNorm(pred_y, maxDepth=1000), 10, 1000) / 1000\n",
        "\n",
        "        # Test time augmentation: mirror image estimate\n",
        "        ##pred_y_flip = scale_up(1, predict(model, x[...,::-1,:]/255, minDepth=10, maxDepth=1000, batch_size=bs)[:,:,:,0]) * 10.0\n",
        "        pred_y_flip = (model.predict(x[...,::-1,:]/255, batch_size=bs)) #*10\n",
        "        pred_y_flip = np.clip(DepthNorm(pred_y_flip, maxDepth=1000), 10, 1000) / 1000\n",
        "\n",
        "        # Crop based on Eigen et al. crop\n",
        "        true_y = true_y[:,crop[0]:crop[1]+1, crop[2]:crop[3]+1]\n",
        "        pred_y = pred_y[:,crop[0]:crop[1]+1, crop[2]:crop[3]+1]\n",
        "        pred_y_flip = pred_y_flip[:,crop[0]:crop[1]+1, crop[2]:crop[3]+1]\n",
        "\n",
        "        # Compute errors per image in batch\n",
        "        for j in range(len(true_y)):\n",
        "            predictions.append((0.5 * pred_y[j]) + (0.5 * np.fliplr(pred_y_flip[j]))   )\n",
        "            testSetDepths.append(true_y[j])\n",
        "\n",
        "    predictions = np.stack(predictions, axis=0)\n",
        "    testSetDepths = np.stack(testSetDepths, axis=0)\n",
        "\n",
        "    e = compute_errors(predictions, testSetDepths)\n",
        "    return e, true_y, pred_y, pred_y_flip,x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dN8-QgN6Gtyl"
      },
      "outputs": [],
      "source": [
        "# Load test data\n",
        "print('Loading test data...', end='')\n",
        "#from data import extract_zip\n",
        "def extract_zip(input_zip):\n",
        "        input_zip=ZipFile(input_zip)\n",
        "        return {name: input_zip.read(name) for name in input_zip.namelist()}\n",
        "data=extract_zip('/content/drive/MyDrive/Colab Notebooks/DenseDepth/nyu_test.zip')\n",
        "\n",
        "rgb = np.load(BytesIO(data['eigen_test_rgb.npy']))\n",
        "depth = np.load(BytesIO(data['eigen_test_depth.npy']))\n",
        "crop = np.load(BytesIO(data['eigen_test_crop.npy']))\n",
        "print('Test data loaded.\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtxtuo3qeaye"
      },
      "outputs": [],
      "source": [
        "#new_model = tf.keras.models.load_model('/content/drive/MyDrive/backup/models/nrdwa_model')\n",
        "#newmodel1  = ADWTNestNet(HEIGHT, WIDTH, color_type=3, num_class=1)\n",
        "#newmodel1.compile(optimizer=opt, loss=depth_loss, metrics=[depth_acc])\n",
        "# Check its architecture\n",
        "#model.summary()\n",
        "#model.load_weights(\"/content/drive/MyDrive/NDWTw3/cp.ckpt\")\n",
        "model.load_weights(\"/content/drive/MyDrive/NDWTw3/cp.ckpt\")\n",
        "print('Model weights loaded.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7zFwvI1J0z3"
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "print('Testing...')\n",
        "\n",
        "e,true_y, pred_y, pred_y_flip,x = evaluate(model, rgb, depth, crop, batch_size=8)\n",
        "#e= compute_errors(labels, preds)\n",
        "print(\"{:>10}, {:>10}, {:>10}, {:>10}, {:>10}, {:>10}\".format('a1', 'a2', 'a3', 'rel', 'rms', 'log_10'))\n",
        "print(\"{:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}\".format(e[0],e[1],e[2],e[3],e[4],e[5]))\n",
        "\n",
        "end = time.time()\n",
        "print('\\nTest time', end-start, 's')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtscPxyTCYEn"
      },
      "outputs": [],
      "source": [
        "  0.0699,     0.1666,     0.3104,     1.7170,     2.1629,     0.3902\n",
        "  0.0699,     0.1666,     0.3104,     1.7170,     2.1629,     0.3902\n",
        "  0.19145378470420837, 0.7555521726608276, 2-3 epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtI2HVLBENY-"
      },
      "source": [
        "## Visualizing model output\n",
        "\n",
        "We visualize the model output over the validation set.\n",
        "The first image is the RGB image, the second image is the ground truth depth map image\n",
        "and the third one is the predicted depth map image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NHOJv3o5gtp"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "preds = model.predict(x_test)\n",
        "\n",
        "for i in range(len(test_paths)-640):\n",
        "  path = partition['test'][i]\n",
        "  label_path = labels[path]\n",
        "  pred = preds[i]\n",
        "  pred = np.squeeze(pred, axis=-1)\n",
        "\n",
        "  plt.subplot(1,3,1)\n",
        "  plt.axis(\"off\")\n",
        "  plt.imshow(pred, cmap=plt.get_cmap('inferno_r'))\n",
        "\n",
        "  plt.subplot(1,3,2)\n",
        "  plt.axis(\"off\")\n",
        "  img = preprocess_depth_map(label_path, horizontal_flip=False)\n",
        "  img = np.squeeze(img, axis=-1)\n",
        "  plt.imshow(img, cmap=plt.get_cmap('inferno_r'))\n",
        "\n",
        "  plt.subplot(1,3,3)\n",
        "  plt.axis(\"off\")\n",
        "  img1 = preprocess_image(path, horizontal_flip=False)\n",
        "  plt.imshow(img1)\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxeWyzfBNISE"
      },
      "source": [
        "### EXPIREMENTING\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhJ4kt_dJEiT"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model, Input\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "def standard_unit(input_tensor, stage, nb_filter, kernel_size=3, batchnorm=True):\n",
        "    prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "\n",
        "    conv_1x1 = prune_low_magnitude(layers.Conv2D(filters=nb_filter, kernel_size=(1, 1),\n",
        "                                                 kernel_initializer='he_normal', padding='same'))(input_tensor)\n",
        "    conv_1x1 = layers.BatchNormalization()(conv_1x1)\n",
        "    conv_1x1 = layers.Activation('relu')(conv_1x1)\n",
        "\n",
        "    conv_3x3 = prune_low_magnitude(layers.Conv2D(filters=nb_filter, kernel_size=(3, 3),\n",
        "                                                 kernel_initializer='he_normal', padding='same'))(input_tensor)\n",
        "    conv_3x3 = layers.BatchNormalization()(conv_3x3)\n",
        "    conv_3x3 = layers.Activation('relu')(conv_3x3)\n",
        "\n",
        "    conv_5x5 = prune_low_magnitude(layers.Conv2D(filters=nb_filter, kernel_size=(5, 5),\n",
        "                                                 kernel_initializer='he_normal', padding='same'))(input_tensor)\n",
        "    conv_5x5 = layers.BatchNormalization()(conv_5x5)\n",
        "    conv_5x5 = layers.Activation('relu')(conv_5x5)\n",
        "\n",
        "    pool_proj = layers.MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(input_tensor)\n",
        "    pool_proj = prune_low_magnitude(layers.Conv2D(filters=nb_filter, kernel_size=(1, 1),\n",
        "                                                  kernel_initializer='he_normal', padding='same'))(pool_proj)\n",
        "    pool_proj = layers.BatchNormalization()(pool_proj)\n",
        "    pool_proj = layers.Activation('relu')(pool_proj)\n",
        "\n",
        "    x = layers.concatenate([conv_1x1, conv_3x3, conv_5x5, pool_proj], axis=-1)\n",
        "    return x\n",
        "\n",
        "def Nest_Net(img_rows, img_cols, color_type=3, num_class=1, deep_supervision=False):\n",
        "    prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "    nb_filter = [32, 64, 128, 256, 512]\n",
        "    bn_axis = 3\n",
        "    img_input = Input(shape=(img_rows, img_cols, color_type), name='main_input')\n",
        "\n",
        "    conv1_1 = standard_unit(img_input, stage='11', nb_filter=nb_filter[0])\n",
        "    pool1 = layers.MaxPooling2D((2, 2), strides=(2, 2), name='pool1')(conv1_1)\n",
        "\n",
        "    conv2_1 = standard_unit(pool1, stage='21', nb_filter=nb_filter[1])\n",
        "    pool2 = layers.MaxPooling2D((2, 2), strides=(2, 2), name='pool2')(conv2_1)\n",
        "\n",
        "    up1_2 = layers.Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up12', padding='same')(conv2_1)\n",
        "    conv1_2 = layers.concatenate([up1_2, conv1_1], name='merge12', axis=bn_axis)\n",
        "    conv1_2 = standard_unit(conv1_2, stage='12', nb_filter=nb_filter[0])\n",
        "\n",
        "    conv3_1 = standard_unit(pool2, stage='31', nb_filter=nb_filter[2])\n",
        "    pool3 = layers.MaxPooling2D((2, 2), strides=(2, 2), name='pool3')(conv3_1)\n",
        "\n",
        "    up2_2 = layers.Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up22', padding='same')(conv3_1)\n",
        "    conv2_2 = layers.concatenate([up2_2, conv2_1], name='merge22', axis=bn_axis)\n",
        "    conv2_2 = standard_unit(conv2_2, stage='22', nb_filter=nb_filter[1])\n",
        "\n",
        "    up1_3 = layers.Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up13', padding='same')(conv2_2)\n",
        "    conv1_3 = layers.concatenate([up1_3, conv1_1, conv1_2], name='merge13', axis=bn_axis)\n",
        "    conv1_3 = standard_unit(conv1_3, stage='13', nb_filter=nb_filter[0])\n",
        "\n",
        "    conv4_1 = standard_unit(pool3, stage='41', nb_filter=nb_filter[3])\n",
        "    pool4 = layers.MaxPooling2D((2, 2), strides=(2, 2), name='pool4')(conv4_1)\n",
        "\n",
        "    up3_2 = layers.Conv2DTranspose(nb_filter[2], (2, 2), strides=(2, 2), name='up32', padding='same')(conv4_1)\n",
        "    conv3_2 = layers.concatenate([up3_2, conv3_1], name='merge32', axis=bn_axis)\n",
        "    conv3_2 = standard_unit(conv3_2, stage='32', nb_filter=nb_filter[2])\n",
        "\n",
        "    up2_3 = layers.Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up23', padding='same')(conv3_2)\n",
        "    conv2_3 = layers.concatenate([up2_3, conv2_1, conv2_2], name='merge23', axis=bn_axis)\n",
        "    conv2_3 = standard_unit(conv2_3, stage='23', nb_filter=nb_filter[1])\n",
        "\n",
        "    up1_4 = layers.Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up14', padding='same')(conv2_3)\n",
        "    conv1_4 = layers.concatenate([up1_4, conv1_1, conv1_2, conv1_3], name='merge14', axis=bn_axis)\n",
        "    conv1_4 = standard_unit(conv1_4, stage='14', nb_filter=nb_filter[0])\n",
        "\n",
        "    conv5_1 = standard_unit(pool4, stage='51', nb_filter=nb_filter[4])\n",
        "\n",
        "    up4_2 = layers.Conv2DTranspose(nb_filter[3], (2, 2), strides=(2, 2), name='up42', padding='same')(conv5_1)\n",
        "    conv4_2 = layers.concatenate([up4_2, conv4_1], name='merge42', axis=bn_axis)\n",
        "    conv4_2 = standard_unit(conv4_2, stage='42', nb_filter=nb_filter[3])\n",
        "\n",
        "    up3_3 = layers.Conv2DTranspose(nb_filter[2], (2, 2), strides=(2, 2), name='up33', padding='same')(conv4_2)\n",
        "    conv3_3 = layers.concatenate([up3_3, conv3_1, conv3_2], name='merge33', axis=bn_axis)\n",
        "    conv3_3 = standard_unit(conv3_3, stage='33', nb_filter=nb_filter[2])\n",
        "\n",
        "    up2_4 = layers.Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up24', padding='same')(conv3_3)\n",
        "    conv2_4 = layers.concatenate([up2_4, conv2_1, conv2_2, conv2_3], name='merge24', axis=bn_axis)\n",
        "    conv2_4 = standard_unit(conv2_4, stage='24', nb_filter=nb_filter[1])\n",
        "\n",
        "    up1_5 = layers.Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up15', padding='same')(conv2_4)\n",
        "    conv1_5 = layers.concatenate([up1_5, conv1_1, conv1_2, conv1_3, conv1_4], name='merge15', axis=bn_axis)\n",
        "    conv1_5 = standard_unit(conv1_5, stage='15', nb_filter=nb_filter[0])\n",
        "\n",
        "    nestnet_output_4 = layers.Conv2D(num_class, (1, 1), activation='sigmoid', name='UNETP',\n",
        "                                     kernel_initializer='he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_5)\n",
        "\n",
        "    if deep_supervision:\n",
        "        model = Model(img_input, [nestnet_output_1, nestnet_output_2, nestnet_output_3, nestnet_output_4])\n",
        "    else:\n",
        "        model = Model(img_input, [nestnet_output_4], name=\"UNetp\")\n",
        "\n",
        "    # Apply pruning step\n",
        "    model = tfmot.sparsity.keras.strip_pruning(model)\n",
        "    return model\n",
        "\n",
        "# Training with pruning\n",
        "def train_pruning_model(model, training_generator, validation_generator, epochs, initial_sparsity, final_sparsity, begin_step, end_step):\n",
        "    pruning_params = {\n",
        "        'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
        "            initial_sparsity=initial_sparsity,\n",
        "            final_sparsity=final_sparsity,\n",
        "            begin_step=begin_step,\n",
        "            end_step=end_step\n",
        "        )\n",
        "    }\n",
        "\n",
        "    model_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(model, **pruning_params)\n",
        "    model_for_pruning.compile(optimizer=opt, loss=depth_loss, metrics=[depth_acc])\n",
        "\n",
        "    cmodel_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(model, **pruning_params)\n",
        "    model_for_pruning.compile(optimizer=opt, loss=depth_loss, metrics=[depth_acc])\n",
        "\n",
        "    pruning_callbacks = [\n",
        "        tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "        tfmot.sparsity.keras.PruningSummaries(log_dir='./logs')\n",
        "    ]\n",
        "\n",
        "    r = model_for_pruning.fit(\n",
        "        training_generator,\n",
        "        validation_data=validation_generator,\n",
        "        epochs=epochs,\n",
        "        callbacks=pruning_callbacks + callbacks  # Combine pruning callbacks with your existing callbacks\n",
        "    )\n",
        "\n",
        "    return model_for_pruning, r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KU5T9mWOJHuw"
      },
      "outputs": [],
      "source": [
        "# Define your model\n",
        "model = Nest_Net(HEIGHT, WIDTH, color_type=3, num_class=1)\n",
        "\n",
        "# Train your model with pruning\n",
        "pruned_model, r = train_pruning_model(\n",
        "    model,\n",
        "    training_generator=training_generator,\n",
        "    validation_generator=validation_generator,\n",
        "    epochs=EPOCHS,\n",
        "    initial_sparsity=0.50,\n",
        "    final_sparsity=0.90,\n",
        "    begin_step=0,\n",
        "    end_step=EPOCHS * len(training_generator)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoTffHvGTDS1"
      },
      "outputs": [],
      "source": [
        "from tensorflow_model_optimization.python.core.sparsity.keras import prune\n",
        "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_schedule\n",
        "from tensorflow_model_optimization.sparsity.keras import UpdatePruningStep\n",
        "\n",
        "\n",
        "def standard_unit(input_tensor, stage, nb_filter, kernel_size=3, batchnorm=True):\n",
        "    # Define the pruning parameters\n",
        "    pruning_params = {\n",
        "        'pruning_schedule': pruning_schedule.ConstantSparsity(\n",
        "            target_sparsity=0.5,  # Adjust based on your experimentation\n",
        "            begin_step=0,\n",
        "            frequency=100\n",
        "        )\n",
        "    }\n",
        "\n",
        "    # 1x1 convolution\n",
        "    conv_1x1 = layers.Conv2D(filters=nb_filter, kernel_size=(1, 1),\n",
        "                             kernel_initializer='he_normal', padding='same',\n",
        "                             **pruning_params)(input_tensor)\n",
        "    conv_1x1 = layers.BatchNormalization()(conv_1x1)\n",
        "    conv_1x1 = layers.Activation('relu')(conv_1x1)\n",
        "\n",
        "    # 3x3 convolution\n",
        "    conv_3x3 = layers.Conv2D(filters=nb_filter, kernel_size=(3, 3),\n",
        "                             kernel_initializer='he_normal', padding='same',\n",
        "                             **pruning_params)(input_tensor)\n",
        "    conv_3x3 = layers.BatchNormalization()(conv_3x3)\n",
        "    conv_3x3 = layers.Activation('relu')(conv_3x3)\n",
        "\n",
        "    # 5x5 convolution\n",
        "    conv_5x5 = layers.Conv2D(filters=nb_filter, kernel_size=(5, 5),\n",
        "                             kernel_initializer='he_normal', padding='same',\n",
        "                             **pruning_params)(input_tensor)\n",
        "    conv_5x5 = layers.BatchNormalization()(conv_5x5)\n",
        "    conv_5x5 = layers.Activation('relu')(conv_5x5)\n",
        "\n",
        "    # 3x3 max pooling followed by 1x1 convolution\n",
        "    pool_proj = layers.MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(input_tensor)\n",
        "    pool_proj = layers.Conv2D(filters=nb_filter, kernel_size=(1, 1),\n",
        "                              kernel_initializer='he_normal', padding='same',\n",
        "                              **pruning_params)(pool_proj)\n",
        "    pool_proj = layers.BatchNormalization()(pool_proj)\n",
        "    pool_proj = layers.Activation('relu')(pool_proj)\n",
        "\n",
        "    # Concatenate all filters\n",
        "    x = layers.concatenate([conv_1x1, conv_3x3, conv_5x5, pool_proj], axis=-1)\n",
        "    return x\n",
        "# Define pruning parameters\n",
        "pruning_params = {\n",
        "    'pruning_schedule': pruning_schedule.ConstantSparsity(\n",
        "        target_sparsity=0.5,  # Adjust based on your experimentation\n",
        "        begin_step=0,\n",
        "        frequency=100\n",
        "    )\n",
        "}\n",
        "\n",
        "# Apply pruning to the entire model\n",
        "LR = 0.001\n",
        "opt = Adam(lr=LR)\n",
        "\n",
        "pruned_model = prune.prune_low_magnitude(model, **pruning_params)\n",
        "\n",
        "# Compile and train the pruned model\n",
        "pruned_model.compile(optimizer=opt, loss=depth_loss, metrics=[depth_acc])\n",
        "callbacks = [\n",
        "    UpdatePruningStep()\n",
        "]\n",
        "r = pruned_model.fit(training_generator, validation_data=validation_generator, epochs=EPOCHS, callbacks=callbacks)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYI56OpmENZD"
      },
      "source": [
        "## References\n",
        "\n",
        "The following papers approaches for depth estimation.\n",
        "1. U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
        "    (http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net)\n",
        "2. UNet++: A Nested U-Net Architecture for Medical Image Segmentation]\n",
        "    (https://arxiv.org/abs/1807.10165)\n",
        "3. Multi-level Wavelet Convolutional Neural Networks\n",
        "    (https://arxiv.org/abs/1907.03128)\n",
        "4. Single Image Depth Prediction with Wavelet Decomposition\n",
        "    (https://github.com/nianticlabs/wavelet-monodepth)\n",
        "5. A Review On Single Image Depth Prediction with Wavelet\n",
        "Decomposition\n",
        "    (https://www.irjet.net/archives/V9/i5/IRJET-V9I5428.pdf)\n",
        "6. Towards Real-Time Monocular Depth Estimation\n",
        "for Robotics: A Survey\n",
        "    (https://arxiv.org/pdf/2111.08600v1.pdf)\n",
        "7. Attention U-Net: Learning Where to Look for the Pancreas\n",
        "    (https://arxiv.org/abs/1804.03999)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
